{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14664279,"sourceType":"datasetVersion","datasetId":9368218}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Negin Heidarifard**  \n**M2 in Artificial Intelligence, Paris-Saclay University**  \n**Course: Computer Vision**  \n**Professor: Dr. Celine Hudelot** \n\n---\n\n\n### Project Introduction\n\nI implemented Eulerian Video Magnification (Wu et al., 2012) as a way to explore how far subtle temporal signals can be recovered from standard video without explicit motion tracking. My initial expectation was that amplifying narrow frequency bands would cleanly expose signals like pulse-related color changes, but in practice the behavior was much more sensitive to parameter choices and noise than anticipated.\n\nSmall changes in the temporal band or amplification factor often led to visible artifacts or amplification of irrelevant motion, especially outside regions of interest. This pushed me to experiment with different filtering strategies and to separate motion amplification from color-only amplification, depending on the type of signal being targeted.\n\nRather than treating EVM as a plug-and-play method, this project focuses on understanding where it works reliably, where it breaks down, and which design choices are critical for keeping the results visually meaningful.\n ","metadata":{}},{"cell_type":"code","source":"# Basic environment setup (Kaggle runtime)\n# Using the default Kaggle Python image for convenience and reproducibility.\n\nimport numpy as np\nimport pandas as pd\n\n# Quick check of available input files\n# Mostly to verify paths before running the pipeline.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Note:\n# Outputs written to /kaggle/working/ are preserved.\n# Temporary files under /kaggle/temp/ are not.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:12:59.311420Z","iopub.execute_input":"2026-01-29T13:12:59.311645Z","iopub.status.idle":"2026-01-29T13:13:00.609358Z","shell.execute_reply.started":"2026-01-29T13:12:59.311606Z","shell.execute_reply":"2026-01-29T13:13:00.608582Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"I keep a reference visualization here mainly as a sanity check while implementing the pipeline.\nI did not rely on this video beyond verifying that the qualitative behavior matches expectations.\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import IFrame\nIFrame('https://www.youtube.com/embed/ONZcjs1Pjmk', width=700, height=350)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:13:00.610671Z","iopub.execute_input":"2026-01-29T13:13:00.611079Z","iopub.status.idle":"2026-01-29T13:13:00.617766Z","shell.execute_reply.started":"2026-01-29T13:13:00.611054Z","shell.execute_reply":"2026-01-29T13:13:00.617064Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7bc5ee2c5eb0>","text/html":"\n        <iframe\n            width=\"700\"\n            height=\"350\"\n            src=\"https://www.youtube.com/embed/ONZcjs1Pjmk\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Imports & Environment Setup\nimport cv2\nimport numpy as np\nimport scipy.fftpack\nimport scipy.signal\nimport os\nimport gc\nfrom IPython.display import FileLink, display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:13:00.618641Z","iopub.execute_input":"2026-01-29T13:13:00.618889Z","iopub.status.idle":"2026-01-29T13:13:02.323072Z","shell.execute_reply.started":"2026-01-29T13:13:00.618865Z","shell.execute_reply":"2026-01-29T13:13:02.322558Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Explanation:**  \nWe import the required libraries:  \n- **cv2** for image/video processing  \n- **numpy** for array manipulation  \n- **scipy.fftpack and scipy.signal** for FFT-based filtering  \n- **os, gc** for file handling and memory management  \n- **IPython.display** for creating download links.  \nThis cell sets up our working environment.\n","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions\n**Explanation:**  \nThese functions handle loading a video (without downsampling) and saving processed videos as AVI files.  \n- **load_video_no_downsample()** reads the video frame by frame and converts the pixel values to the [0,1] range.  \n- **save_video_float32_as_avi()** converts the processed float32 video back to uint8 and writes it using the XVID codec.\n","metadata":{}},{"cell_type":"code","source":"# Cell 3: Helper Functions\n\ndef load_video_no_downsample(video_filename):\n    \"\"\"\n    Loads the full video into memory without downsampling.\n    This turned out to be memory-heavy for longer videos.\n    \"\"\"\n\n    if not os.path.isfile(video_filename):\n        raise FileNotFoundError(f\"Video not found: {video_filename}\")\n    cap = cv2.VideoCapture(video_filename)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = []\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_f = frame_bgr.astype(np.float32) / 255.0\n        frames.append(frame_f)\n    cap.release()\n    video_array = np.array(frames, dtype=np.float32)\n    return video_array, fps\n\ndef save_video_float32_as_avi(video_data, fps, out_filename=\"output.avi\"):\n    \"\"\"\n    Writes output video to disk.\n    \"\"\"\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    h, w = video_data.shape[1:3]\n    out = cv2.VideoWriter(out_filename, fourcc, fps, (w, h), True)\n    for i in range(video_data.shape[0]):\n        frame_uint8 = np.clip(video_data[i] * 255.0, 0, 255).astype(np.uint8)\n        out.write(frame_uint8)\n    out.release()\n    print(f\"Saved {out_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:13:53.969055Z","iopub.execute_input":"2026-01-29T13:13:53.969608Z","iopub.status.idle":"2026-01-29T13:13:53.977809Z","shell.execute_reply.started":"2026-01-29T13:13:53.969578Z","shell.execute_reply":"2026-01-29T13:13:53.976935Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Eulerian Motion Magnification\n\nThis block contains the core implementation of the Eulerian pipeline.\nMost of the complexity here comes from handling pyramid boundaries and keeping shapes consistent across levels, which turned out to be more error-prone than expected.\n","metadata":{}},{"cell_type":"code","source":"# Core EVM implementation\n\n\ndef create_laplacian_pyramid_frame(frame, pyramid_levels=4):\n    gauss_pyr = [frame]\n    for _ in range(1, pyramid_levels):\n        gauss_pyr.append(cv2.pyrDown(gauss_pyr[-1]))\n    lap_pyr = []\n    for i in range(pyramid_levels - 1):\n        up = cv2.pyrUp(gauss_pyr[i+1])\n        h, w = gauss_pyr[i].shape[:2]\n        up = up[:h, :w]\n        lap_pyr.append(gauss_pyr[i] - up)\n    lap_pyr.append(gauss_pyr[-1])\n    return lap_pyr\n\ndef create_laplacian_video_pyramid(video, pyramid_levels=4):\n    nframes = video.shape[0]\n    pyramid = None\n    for i in range(nframes):\n        frame_pyr = create_laplacian_pyramid_frame(video[i], pyramid_levels)\n        if pyramid is None:\n            pyramid = []\n            for lvl in range(pyramid_levels):\n                lvl_h, lvl_w = frame_pyr[lvl].shape[:2]\n                pyramid.append(np.zeros((nframes, lvl_h, lvl_w, 3), dtype=np.float32))\n        for lvl in range(pyramid_levels):\n            pyramid[lvl][i] = frame_pyr[lvl]\n    return pyramid\n\ndef collapse_laplacian_pyramid_frame(lap_pyr):\n    output = lap_pyr[-1]\n    for lvl in reversed(range(len(lap_pyr) - 1)):\n        up = cv2.pyrUp(output)\n        h, w = lap_pyr[lvl].shape[:2]\n        up = up[:h, :w]\n        output = lap_pyr[lvl] + up\n    return output\n\ndef collapse_laplacian_video_pyramid(pyramid):\n    nframes = pyramid[0].shape[0]\n    collapsed_frames = []\n    for i in range(nframes):\n        lap_pyr_frame = [pyramid[lvl][i] for lvl in range(len(pyramid))]\n        collapsed_frame = collapse_laplacian_pyramid_frame(lap_pyr_frame)\n        collapsed_frames.append(collapsed_frame)\n    return np.array(collapsed_frames, dtype=np.float32)\n\ndef temporal_bandpass_filter(data, fps, freq_min, freq_max, amplification=1.0, axis=0):\n    fft_data = scipy.fftpack.rfft(data, axis=axis)\n    freqs = scipy.fftpack.rfftfreq(data.shape[0], d=1.0/fps)\n    low_idx = np.argmin(np.abs(freqs - freq_min))\n    high_idx = np.argmin(np.abs(freqs - freq_max))\n    fft_data[:low_idx] = 0\n    fft_data[high_idx+1:] = 0\n    filtered = scipy.fftpack.irfft(fft_data, axis=axis)\n    filtered *= amplification\n    return filtered\n\ndef eulerian_magnification(vid_data, fps, freq_min, freq_max, amplification,\n                           pyramid_levels=4, skip_levels_at_top=1):\n    vid_pyr = create_laplacian_video_pyramid(vid_data, pyramid_levels)\n    for lvl in range(len(vid_pyr)):\n        if lvl < skip_levels_at_top or lvl == len(vid_pyr) - 1:\n            continue\n        bandpassed = temporal_bandpass_filter(vid_pyr[lvl], fps, freq_min, freq_max, amplification, axis=0)\n        vid_pyr[lvl] += bandpassed\n    return collapse_laplacian_video_pyramid(vid_pyr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:16:03.606668Z","iopub.execute_input":"2026-01-29T13:16:03.606957Z","iopub.status.idle":"2026-01-29T13:16:03.618459Z","shell.execute_reply.started":"2026-01-29T13:16:03.606931Z","shell.execute_reply":"2026-01-29T13:16:03.617652Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Eulerian Color Amplification\n\nI switched to a color-only formulation when motion amplification proved unstable under small parameter changes.\n","metadata":{}},{"cell_type":"code","source":"def eulerian_color_amplification(vid_data, fps, freq_min, freq_max, amplification, pyramid_levels=4):\n    \"\"\"\n    Color-only variant used when motion amplification introduced visible artifacts.\n    This approach turned out to be highly sensitive to frequency bounds and amplification.\n    \"\"\"\n    nframes, orig_h, orig_w, _ = vid_data.shape\n    gauss_frames = []\n    for i in range(nframes):\n        frame = vid_data[i]\n        for _ in range(pyramid_levels - 1):\n            frame = cv2.pyrDown(frame)\n        gauss_frames.append(frame)\n\n    gauss_video = np.array(gauss_frames, dtype=np.float32)\n    bandpassed = temporal_bandpass_filter(\n        gauss_video, fps, freq_min, freq_max, amplification, axis=0\n    )\n    filtered_coarse = gauss_video + bandpassed\n\n    up_frames = []\n    for i in range(nframes):\n        up_frame = filtered_coarse[i]\n        for _ in range(pyramid_levels - 1):\n            up_frame = cv2.pyrUp(up_frame)\n        up_frame = up_frame[:orig_h, :orig_w]\n        amplified_frame = np.clip(vid_data[i] + up_frame, 0, 1)\n        up_frames.append(amplified_frame)\n\n    return np.array(up_frames, dtype=np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:19:16.625933Z","iopub.execute_input":"2026-01-29T13:19:16.626658Z","iopub.status.idle":"2026-01-29T13:19:16.632377Z","shell.execute_reply.started":"2026-01-29T13:19:16.626627Z","shell.execute_reply":"2026-01-29T13:19:16.631615Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Parameter Search\n\nIn practice, the behavior of Eulerian magnification was highly sensitive to frequency bounds and amplification strength. Small changes often led to either no visible signal or strong artifacts, making manual tuning unreliable.\n\nTo reduce trial-and-error, I implemented a simple grid search over a narrow parameter range. The objective used here is intentionally crude and only meant to provide a relative comparison between settings, not a robust physiological estimate.\n\nThis utility helped narrow down reasonable parameter ranges, but the results were not always stable across videos or regions of interest.\n","metadata":{}},{"cell_type":"code","source":"# Parameter search (heuristic)\n\ndef compute_objective(vid_data, fps, freq_min, freq_max, amplification, roi=(0, 50, 0, 50)):\n    \"\"\"\n    Crude objective for comparing parameter settings.\n    This is not a reliable physiological metric and is sensitive to ROI choice and noise.\n    \"\"\"\n    # Assumes Eulerian magnification is already defined\n    processed = eulerian_magnification(\n        vid_data,\n        fps,\n        freq_min,\n        freq_max,\n        amplification,\n        pyramid_levels=5,\n        skip_levels_at_top=1\n    )\n\n    # Aggregate signal inside the ROI over time\n    x1, x2, y1, y2 = roi\n    roi_signal = processed[:, y1:y2, x1:x2, :].sum(axis=(1, 2, 3))\n\n    # Frequency-domain analysis of the aggregated signal\n    fft_vals = np.abs(np.fft.fft(roi_signal))\n    freqs = np.fft.fftfreq(len(roi_signal), d=1.0 / fps)\n\n    # Keep only positive frequencies\n    pos_mask = freqs > 0\n    fft_vals = fft_vals[pos_mask]\n    freqs = freqs[pos_mask]\n\n    # Measure energy in a narrow target band\n    band_mask = (freqs >= 0.7) & (freqs <= 1.2)\n    if not np.any(band_mask):\n        return 0.0\n\n    return np.max(fft_vals[band_mask])\n\n\ndef grid_search_eulerian_params(vid_data, fps, roi=(0, 50, 0, 50)):\n    \"\"\"\n    Simple brute-force search over a small parameter grid.\n    Used to narrow down unstable regions of the parameter space.\n    \"\"\"\n    freq_min_vals = [0.7, 0.75, 0.8]\n    freq_max_vals = [1.0, 1.05, 1.1]\n    amp_vals = [70, 80, 90]\n\n    best_score = -float(\"inf\")\n    best_params = None\n\n    for fmin in freq_min_vals:\n        for fmax in freq_max_vals:\n            if fmin >= fmax:\n                continue\n            for amp in amp_vals:\n                score = compute_objective(vid_data, fps, fmin, fmax, amp, roi)\n                if score > best_score:\n                    best_score = score\n                    best_params = (fmin, fmax, amp)\n\n    return best_params, best_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:23:46.593248Z","iopub.execute_input":"2026-01-29T13:23:46.593739Z","iopub.status.idle":"2026-01-29T13:23:46.600735Z","shell.execute_reply.started":"2026-01-29T13:23:46.593715Z","shell.execute_reply":"2026-01-29T13:23:46.600175Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Testing and Observations\n\nI evaluated the pipeline on a small set of short videos with different signal characteristics (infant breathing, facial color changes, wrist pulse). In practice, the behavior varied significantly across videos, and a single parameter setting did not generalize well.\n\nFor videos dominated by subtle color variations, the color-only amplification was more stable than full motion amplification, which often introduced visible artifacts. Parameter values reported below were chosen empirically after several failed or unstable runs, and should not be interpreted as universally optimal.\n\nThe goal of this section is to qualitatively assess when the method produces interpretable results and when it breaks down, rather than to report quantitative performance.\n","metadata":{}},{"cell_type":"code","source":"# Testing on a small set of videos with empirically chosen parameters\n\n# Base path for local testing (Kaggle dataset)\nbase_path = \"/kaggle/input/cv-eulerian-videos\"\n\n# Per-video parameters tuned empirically.\n# Color-only amplification is used where motion amplification was unstable.\nvideos = {\n    \"baby\": {\n        \"path\": os.path.join(base_path, \"baby.mp4\"),\n        \"params\": {\n            \"freq_min\": 0.8,\n            \"freq_max\": 2.0,\n            \"amplification\": 50,\n            \"pyramid_levels\": 4,\n            \"skip_levels_at_top\": 1\n        },\n        \"color_amp\": False\n    },\n    \"baby2\": {\n        \"path\": os.path.join(base_path, \"baby2.mp4\"),\n        \"params\": {\n            \"freq_min\": 2.0,\n            \"freq_max\": 2.5,\n            \"amplification\": 100,\n            \"pyramid_levels\": 4\n        },\n        \"color_amp\": True\n    },\n    \"face\": {\n        \"path\": os.path.join(base_path, \"face.mp4\"),\n        \"params\": {\n            \"freq_min\": 0.8,\n            \"freq_max\": 1.0,\n            \"amplification\": 80,\n            \"pyramid_levels\": 5\n        },\n        \"use_grid_search\": True,\n        \"roi\": (50, 150, 40, 120),\n        \"color_amp\": True\n    },\n    \"wrist\": {\n        \"path\": os.path.join(base_path, \"wrist.mp4\"),\n        \"params\": {\n            \"freq_min\": 0.4,\n            \"freq_max\": 3.0,\n            \"amplification\": 15,\n            \"pyramid_levels\": 4,\n            \"skip_levels_at_top\": 1\n        },\n        \"color_amp\": False\n    }\n}\n\n# Process each video sequentially and save the result\nfor vid_name, vid_info in videos.items():\n    print(f\"\\nProcessing {vid_name} ...\")\n    video_path = vid_info[\"path\"]\n\n    if not os.path.isfile(video_path):\n        print(f\"File not found: {video_path}\")\n        continue\n\n    try:\n        vid_data, fps = load_video_no_downsample(video_path)\n        print(f\"{vid_name} loaded: shape={vid_data.shape}, fps={fps}\")\n    except Exception as e:\n        print(f\"Error loading {vid_name}: {e}\")\n        continue\n\n    params = vid_info[\"params\"]\n\n    if vid_info.get(\"use_grid_search\", False):\n        # Grid search used only to narrow down unstable parameter regions\n        from IPython.display import clear_output\n        print(\"Running grid search ...\")\n        best_params, best_score = grid_search_eulerian_params(\n            vid_data,\n            fps,\n            roi=vid_info.get(\"roi\", (0, 50, 0, 50))\n        )\n        if best_params:\n            params[\"freq_min\"], params[\"freq_max\"], params[\"amplification\"] = best_params\n        clear_output(wait=True)\n\n    if vid_info.get(\"color_amp\", False):\n        # Color-only amplification (motion amplification caused artifacts here)\n        magnified = eulerian_color_amplification(\n            vid_data,\n            fps,\n            freq_min=params[\"freq_min\"],\n            freq_max=params[\"freq_max\"],\n            amplification=params[\"amplification\"],\n            pyramid_levels=params[\"pyramid_levels\"]\n        )\n    else:\n        # Standard Eulerian motion magnification\n        magnified = eulerian_magnification(\n            vid_data,\n            fps,\n            freq_min=params[\"freq_min\"],\n            freq_max=params[\"freq_max\"],\n            amplification=params[\"amplification\"],\n            pyramid_levels=params[\"pyramid_levels\"],\n            skip_levels_at_top=params.get(\"skip_levels_at_top\", 1)\n        )\n\n    out_filename = f\"magnified_{vid_name}.avi\"\n    save_video_float32_as_avi(magnified, fps, out_filename)\n\n    # Explicit cleanup to limit memory growth across runs\n    del vid_data, magnified\n    gc.collect()\n\nprint(\"\\nAll processing completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:41:33.855009Z","iopub.execute_input":"2026-01-29T13:41:33.855765Z","iopub.status.idle":"2026-01-29T13:44:33.176484Z","shell.execute_reply.started":"2026-01-29T13:41:33.855736Z","shell.execute_reply":"2026-01-29T13:44:33.175844Z"}},"outputs":[{"name":"stdout","text":"Saved magnified_face.avi\n\nProcessing wrist ...\nwrist loaded: shape=(894, 352, 640, 3), fps=30.0\nSaved magnified_wrist.avi\n\nAll processing completed.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Access processed outputs for qualitative inspection\n\n# Explicit list of generated result files\noutput_files = [\n    \"magnified_baby.avi\",\n    \"magnified_baby2.avi\",\n    \"magnified_face.avi\",\n    \"magnified_wrist.avi\"\n]\n\n# Provide direct links to the generated videos\nfor file in output_files:\n    display(FileLink(file))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T13:44:59.729561Z","iopub.execute_input":"2026-01-29T13:44:59.730064Z","iopub.status.idle":"2026-01-29T13:44:59.738693Z","shell.execute_reply.started":"2026-01-29T13:44:59.730038Z","shell.execute_reply":"2026-01-29T13:44:59.738119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_baby.avi","text/html":"<a href='magnified_baby.avi' target='_blank'>magnified_baby.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_baby2.avi","text/html":"<a href='magnified_baby2.avi' target='_blank'>magnified_baby2.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_face.avi","text/html":"<a href='magnified_face.avi' target='_blank'>magnified_face.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_wrist.avi","text/html":"<a href='magnified_wrist.avi' target='_blank'>magnified_wrist.avi</a><br>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## Analysis of *baby.mp4*\n\nWhen I started working with *baby.mp4*, my main question was whether Eulerian motion magnification could make the infant’s breathing visible without damaging the overall visual quality of the video. In the raw footage, the motion is extremely subtle; at first glance, the breathing is almost imperceptible unless you know exactly where to look.\n\nI initially had some doubts about the parameter choices, especially the amplification factor. An amplification of 50 felt relatively high, and I expected it might introduce visible artifacts or edge distortions, particularly around the baby’s body. To mitigate this, I decided to skip the finest pyramid level, since early experiments showed that this level tended to amplify noise rather than meaningful motion.\n\nAfter applying the magnification with a frequency band between 0.8 Hz and 2.0 Hz, the breathing pattern became noticeably easier to perceive. The motion is still subtle, but it is now clearly distinguishable from background noise. Importantly, the image remains visually stable: I did not observe obvious halos, flickering, or spatial distortions, which suggests that this parameter combination strikes a reasonable balance between amplification strength and visual fidelity.\n\nOne thing I noticed is that the result is highly dependent on the infant remaining relatively still. The temporal consistency of the amplified motion looks good, and the breathing appears smooth, which indicates that the temporal filtering behaves as intended. However, I expect that larger global movements would quickly dominate the signal and reduce the usefulness of this setup.\n\nOverall, this experiment shows that Eulerian magnification can enhance subtle physiological motion in infant videos in a qualitative sense. With the chosen parameters, the breathing becomes more visible while preserving a natural appearance, making the output potentially useful for non-intrusive visual monitoring rather than precise measurement.\n","metadata":{}},{"cell_type":"markdown","source":"## Analysis of *wrist.mp4*\n\nFor the wrist video, the main question was whether Eulerian motion magnification could make the pulse-related motion visible without introducing distracting artifacts. In the original *wrist.mp4*, the wrist appears mostly static, and any pulsating motion is extremely subtle and easy to miss unless closely inspected.\n\nBefore processing, I was cautious about the amplification strength. Unlike the infant video, wrist motion is more localized and sensitive to noise, so I deliberately chose a lower amplification factor. An amplification of 15 felt like a safer starting point, as early trials with higher values tended to exaggerate small lighting variations rather than the pulse itself. Skipping the top pyramid level was again helpful in preventing high-frequency noise from dominating the result.\n\nAfter applying Eulerian magnification with a frequency range between 0.4 Hz and 3.0 Hz, the pulsating motion in the wrist became more noticeable. The enhancement remains subtle, but the rhythmic pattern is clearer and easier to follow over time compared to the original video. This suggests that the selected frequency band captures the intended physiological signal reasonably well.\n\nIn terms of visual quality, the processed video stays stable. I did not observe strong artifacts or visible degradation, which indicates that the chosen parameters are appropriate for this type of motion. The amplified signal is temporally smooth, and the pulse appears consistent across frames, suggesting that the temporal filtering is effectively isolating the relevant frequency content.\n\nOverall, the enhanced video preserves a natural appearance while making the wrist pulse easier to observe. This makes the output suitable for qualitative inspection and exploratory analysis, particularly in contexts where non-intrusive visualization of physiological signals is desirable.\n","metadata":{}},{"cell_type":"markdown","source":"## Analysis of *face.mp4*\n\nWhen working with *face.mp4*, my main objective was to see whether Eulerian Video Magnification could make very subtle facial color variations visible, particularly those that might be linked to physiological signals such as heart rate or blood flow. In the original video, the face appears visually stable, with only minimal and barely perceptible color fluctuations across frames.\n\nEarly on, I decided to focus exclusively on color amplification rather than motion. Previous experiments showed that applying motion magnification to facial videos often introduces distracting artifacts, especially around edges and expressions, which can easily dominate the signal of interest. For this reason, the processing was restricted to color changes only.\n\nThe frequency band was set between 0.8 Hz and 1.0 Hz to roughly match the resting heart rate of an adult. I chose an amplification factor of 80 somewhat cautiously. While lower values tended to make the effect almost invisible, higher values quickly led to exaggerated color shifts that no longer looked natural. Using a five-level pyramid helped stabilize the reconstruction, and restricting the analysis to a specific region of interest on the face ((50,150,40,120)) made the color variations easier to interpret by avoiding irrelevant background regions.\n\nAfter processing, subtle color fluctuations become more apparent in the selected facial area. These variations are not dramatic, but they are noticeably more visible than in the original video and appear temporally consistent. Importantly, the overall facial structure and expressions remain intact, and I did not observe strong artifacts or distortions that would compromise realism.\n\nThe amplified signal looks smooth over time, suggesting that the temporal filtering is isolating the intended frequency band reasonably well. While the result is still qualitative, the enhanced visibility of these color changes suggests potential for non-contact monitoring of physiological signals.\n\nOverall, *magnified_face.avi* demonstrates that color-based Eulerian magnification can highlight subtle facial color variations while preserving a natural appearance. This makes the approach promising for exploratory applications in non-invasive health monitoring, where visual interpretability is more important than precise numerical measurement.\n","metadata":{}},{"cell_type":"markdown","source":"## Analysis of *baby2.mp4*\n\nFor *baby2.mp4*, the focus shifted toward detecting subtle color variations rather than motion, with the assumption that these variations could be linked to physiological signals such as pulse in a newborn. In the original video, these changes are extremely faint and not easily distinguishable by visual inspection alone.\n\nGiven the higher heart rate typically observed in newborns, I selected a relatively narrow frequency band between 2.0 Hz and 2.5 Hz. This choice felt slightly aggressive at first, especially when combined with a high amplification factor. An amplification of 100 raised concerns about potential color saturation or unstable visual artifacts, but lower values tended to make the signal almost invisible. Using a four-level pyramid was a compromise between preserving spatial detail and keeping the processing stable.\n\nTo avoid amplifying motion-related artifacts, I relied exclusively on color-only amplification. Each frame was downsampled to obtain a coarse color representation, temporally filtered within the target frequency band, and then reconstructed back to the original resolution. This approach proved more robust than full motion amplification for this particular video.\n\nAfter processing, rhythmic color variations become noticeably more visible, particularly in regions corresponding to the newborn’s face. These variations are barely perceptible in the original footage but emerge clearly in the processed output. Despite the relatively high amplification factor, the visual quality remains acceptable, and I did not observe strong artifacts or distracting distortions.\n\nThe amplified color signal appears smooth and temporally consistent, which suggests that the temporal bandpass filtering successfully isolates the intended frequency components. While the result remains qualitative, the enhanced visibility of these subtle color changes indicates that this parameter configuration is effective for exploratory analysis.\n\nOverall, *magnified_baby2.avi* demonstrates that color-based Eulerian magnification can reveal physiological color signals in newborn videos when appropriate frequency ranges and amplification levels are used. The method preserves a natural visual appearance while making otherwise imperceptible signals easier to observe, supporting its potential use in non-invasive health monitoring contexts.\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2 – My Own Dataset  \n## Testing Eulerian Color Amplification on My Own Video\n\n### Subtle Color Changes Detection with Optimized Parameters\n\n---\n\n### Objective\n\nIn this part, I applied **Eulerian Color Amplification (ECA)** to my own recording (*face3.mp4*) with the goal of visualizing **very subtle facial color variations** that are not noticeable in the raw video.  \nThe main difficulty here was finding a parameter set that enhances physiological signals clearly, without pushing the amplification to the point where the face looks unnatural or noisy.\n\n---\n\n### Methodology Overview\n\nThe processing pipeline follows the standard ECA framework, with a few practical choices motivated by empirical testing rather than theory alone:\n\n1. **Preprocessing**\n\n   - The video was first **downsampled to 640×360**, mainly to reduce memory usage and make experimentation faster.\n   - A light **Gaussian denoising** step was added, since early tests showed that high-frequency noise tends to get amplified together with the signal.\n   - The sequence was limited to **300 frames**, which turned out to be sufficient to capture periodic physiological variations while keeping the computation manageable.\n\n2. **Temporal Bandpass Filtering**\n\n   - A frequency band of **0.7–1.2 Hz** was selected, corresponding to a typical adult resting heart rate.\n   - An **amplification factor of 30** was chosen after testing higher values, which produced stronger signals but also introduced visible color artifacts.\n\n3. **Color-Only Amplification**\n\n   - A **5-level pyramid decomposition** was used to capture subtle color changes at an appropriate spatial scale.\n   - During reconstruction, **cubic interpolation** was applied to keep the upsampling visually smooth and stable.\n\n4. **Reconstruction and Saving**\n\n   - The amplified color signal was added back to the original frames.\n   - The final result was saved in **AVI format using the XVID codec**, ensuring smooth playback and reasonable file size.\n\n---\n\n### Key Parameters and Rationale\n\n- **Frequency Range (0.7–1.2 Hz):** Chosen to target physiological color variations related to blood flow.\n- **Amplification Factor (30):** A compromise between visibility of the signal and visual realism.\n- **Pyramid Levels (5):** Provides enough spatial detail without significantly increasing computation time.\n- **Gaussian Denoising:** Helps prevent the amplification of irrelevant noise components.\n\n---\n\n### Output Details\n\n- **Output File:** `magnified_face3_lower_amp.avi`  \n- **Video Codec:** XVID  \n- **Resolution:** 640×360  \n\nOverall, the output highlights **subtle facial color changes** while preserving a **natural appearance** and avoiding excessive noise, which is crucial for any realistic monitoring scenario.\n\n---\n\n### Conclusion\n\nThis experiment shows that **Eulerian Color Amplification**, when carefully tuned, can successfully reveal subtle facial color variations in a real, unconstrained video.  \nThe chosen parameters produce a stable and visually coherent result, demonstrating the practical usefulness of ECA for **non-invasive physiological signal visualization**.\n","metadata":{}},{"cell_type":"code","source":"# =======================================================\n# Final Experiment: Subtle Color Amplification on \"face3.mp4\"\n# (Carefully tuned for realism rather than strong amplification)\n# =======================================================\n\n# Step 1: Load video with practical constraints in mind\n# - Downsampling for memory efficiency\n# - Mild denoising to avoid amplifying high-frequency noise\n# - Frame limiting to keep computation reasonable\n# -------------------------------------------------------\ndef load_video_downsampled_denoise(video_path, max_frames=300, width=640, height=360):\n    if not os.path.isfile(video_path):\n        raise FileNotFoundError(f\"Video not found: {video_path}\")\n\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    frames = []\n    count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret or count >= max_frames:\n            break\n\n        # Resize early to reduce memory usage\n        frame = cv2.resize(frame, (width, height))\n\n        # Light Gaussian blur: enough to suppress sensor noise,\n        # but not strong enough to wash out color information\n        frame = cv2.GaussianBlur(frame, (3, 3), 0)\n\n        frames.append(frame.astype(np.float32) / 255.0)\n        count += 1\n\n    cap.release()\n    return np.array(frames, dtype=np.float32), fps\n\n\n# -------------------------------------------------------\n# Step 2: Temporal bandpass filtering\n# Isolates frequency components related to physiological signals\n# -------------------------------------------------------\ndef temporal_bandpass_filter(data, fps, freq_min, freq_max, amplification=1.0):\n    fft_data = scipy.fftpack.rfft(data, axis=0)\n    freqs = scipy.fftpack.rfftfreq(data.shape[0], d=1.0 / fps)\n\n    # Identify frequency indices corresponding to the target band\n    low_idx = np.argmin(np.abs(freqs - freq_min))\n    high_idx = np.argmin(np.abs(freqs - freq_max))\n\n    # Suppress everything outside the band of interest\n    fft_data[:low_idx] = 0\n    fft_data[high_idx + 1:] = 0\n\n    return scipy.fftpack.irfft(fft_data, axis=0) * amplification\n\n\n# -------------------------------------------------------\n# Step 3: Eulerian Color Amplification (low-amplitude version)\n# This variant focuses on subtle color changes rather than motion\n# -------------------------------------------------------\ndef eulerian_color_amplification_improved(\n    vid, fps, freq_min, freq_max, amplification, pyramid_levels=5\n):\n    nframes, orig_h, orig_w, _ = vid.shape\n\n    # Build a coarse representation using repeated downsampling\n    coarse_frames = []\n    for i in range(nframes):\n        frame = vid[i]\n        for _ in range(pyramid_levels - 1):\n            frame = cv2.pyrDown(frame)\n        coarse_frames.append(frame)\n\n    coarse_video = np.array(coarse_frames, dtype=np.float32)\n\n    # Apply temporal filtering on the coarse signal\n    filtered = coarse_video + temporal_bandpass_filter(\n        coarse_video, fps, freq_min, freq_max, amplification\n    )\n\n    # Upsample back to original resolution and combine with input\n    up_frames = []\n    for i in range(nframes):\n        up_frame = cv2.resize(\n            filtered[i], (orig_w, orig_h), interpolation=cv2.INTER_CUBIC\n        )\n        combined = np.clip(vid[i] + up_frame, 0, 1)\n        up_frames.append(combined)\n\n    return np.array(up_frames, dtype=np.float32)\n\n\n# -------------------------------------------------------\n# Step 4: Save the processed video to disk\n# -------------------------------------------------------\ndef save_video(video, fps, filename=\"output.avi\"):\n    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n    h, w = video.shape[1:3]\n    out = cv2.VideoWriter(filename, fourcc, fps, (w, h), True)\n\n    for frame in video:\n        out.write(np.clip(frame * 255.0, 0, 255).astype(np.uint8))\n\n    out.release()\n    print(f\"Saved {filename}\")\n\n\n# -------------------------------------------------------\n# Step 5: Run the full pipeline on face3.mp4\n# Parameters are intentionally conservative to preserve realism\n# -------------------------------------------------------\n\nvideo_path = \"/kaggle/input/cv-eulerian-videos/face3.mp4\"\n\nvid_data, fps = load_video_downsampled_denoise(\n    video_path, max_frames=300, width=640, height=360\n)\n\nprint(f\"Loaded face3.mp4: shape={vid_data.shape}, fps={fps:.2f}\")\n\nfreq_min, freq_max = 0.7, 1.2      # Typical adult heart-rate band\namplification = 30                 # Lower amplification to avoid artifacts\npyramid_levels = 5\n\nresult = eulerian_color_amplification_improved(\n    vid_data, fps, freq_min, freq_max, amplification, pyramid_levels\n)\n\nsave_video(result, fps, \"magnified_face3_lower_amp.avi\")\n\ndel vid_data, result\ngc.collect()\n\n\n# -------------------------------------------------------\n# Step 6: Provide a download link for the final output\n# -------------------------------------------------------\ndisplay(FileLink(\"magnified_face3_lower_amp.avi\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T14:01:46.636340Z","iopub.execute_input":"2026-01-29T14:01:46.636923Z","iopub.status.idle":"2026-01-29T14:01:51.300284Z","shell.execute_reply.started":"2026-01-29T14:01:46.636893Z","shell.execute_reply":"2026-01-29T14:01:51.299717Z"}},"outputs":[{"name":"stdout","text":"Loaded face3.mp4: shape=(300, 360, 640, 3), fps=30.07\nSaved magnified_face3_lower_amp.avi\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_face3_lower_amp.avi","text/html":"<a href='magnified_face3_lower_amp.avi' target='_blank'>magnified_face3_lower_amp.avi</a><br>"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## Comparative Analysis of Original and Amplified Face Video\n\n### Context and Goal  \nI applied Eulerian Color Amplification to *face3.mp4* with the intention of checking whether very subtle, physiology-related color variations could be made visible without harming the visual realism of the video. The goal was deliberately conservative: instead of pushing the amplification aggressively, I wanted to see if a **low-amplitude, carefully tuned setup** could still reveal meaningful signals while keeping noise and artifacts under control.\n\nThe comparison focuses on how the processed output (*magnified_face3_lower_amp (1).avi*) behaves visually relative to the original video, rather than on any quantitative metric alone.\n\n---\n\n### How the Comparison Was Performed  \n- The **original video (*face3.mp4*)** was used as a baseline to assess lighting stability, natural skin tones, and overall image quality.  \n- The **processed video (*magnified_face3_lower_amp (1).avi*)** was then examined to see whether subtle color fluctuations, invisible in the raw footage, became perceptible after amplification.\n\n---\n\n### Observations and Findings  \n\n#### Visual Quality  \nIn the original video, lighting conditions are stable and skin tones appear natural, with no obvious color flicker. After amplification, the overall resolution and sharpness are preserved. I initially expected that even a modest amplification might introduce visible noise or pixel-level artifacts, but in practice the image remains clean and visually stable.\n\n#### Detection of Subtle Color Changes  \nThe processed video reveals faint but coherent color variations across the facial region. These changes are subtle enough that they do not distract from the original appearance, yet they are clearly more noticeable than in the unprocessed video. Without amplification, these variations are essentially imperceptible, which suggests that the chosen parameters are effectively targeting signals that were present but hidden.\n\n#### Noise and Artifacts  \nOne concern was that amplifying color signals would also amplify sensor noise or compression artifacts. This turned out to be less problematic than anticipated. Noise levels remain low, and there are no obvious ringing effects or color bleeding. The mild Gaussian denoising applied during preprocessing likely played an important role in stabilizing the result.\n\n#### Temporal Consistency  \nThe amplified color changes evolve smoothly over time, without abrupt jumps or temporal instability. This suggests that the temporal bandpass filter was well aligned with the target frequency range. Importantly, the rhythm of the color variations appears consistent with expected physiological behavior, which makes the result more convincing from a qualitative perspective.\n\n---\n\n### Conclusion  \nThe processed video *magnified_face3_lower_amp (1).avi* demonstrates that Eulerian Color Amplification can enhance subtle physiological color variations while preserving the natural appearance of the face. The combination of a narrow frequency band, moderate amplification, and multi-scale processing appears to be a good compromise between visibility and realism.\n\nRather than dramatically altering the video, this configuration reveals information that was already present but visually inaccessible, which aligns well with the original motivation behind Eulerian approaches.\n\n---\n\n### Limitations and Next Steps  \nThis analysis is qualitative and relies on visual inspection, which means it cannot confirm whether the observed color variations correspond exactly to physiological measurements. Future work could include controlled experiments under different lighting conditions, comparisons across subjects, or validation against ground-truth signals such as heart rate sensors.\n\nFrom an application perspective, exploring real-time implementations or adaptive parameter tuning could further improve robustness in less controlled settings.\n","metadata":{}},{"cell_type":"markdown","source":"## Extended Eulerian Video Magnification: From a Basic Pipeline to a Paper-Aligned Implementation\n\nWhen I first implemented Eulerian Video Magnification (EVM), the goal was mainly to get a working pipeline and understand the core idea: amplifying subtle temporal variations without explicitly estimating motion. As the experiments progressed, it became clear that the initial implementation, while functional, was far from the level of robustness and visual quality described in the original work by Wu et al. (2012).\n\nTo address this gap, the pipeline was gradually refined by incorporating additional components inspired directly by the paper:\n\n> H. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman,  \n> *Eulerian Video Magnification for Revealing Subtle Changes in the World*,  \n> ACM Transactions on Graphics (Proc. SIGGRAPH), 2012.\n\nWhat follows is not just a feature comparison, but a reflection on how and why each step evolved from a minimal implementation toward something closer to the approach proposed in the paper.\n\n---\n\n## Early Implementation vs. Expanded Approach\n\n### Initial Version\n\nThe earliest version of the pipeline focused on simplicity. Video frames were read, normalized, temporally filtered, and written back with minimal preprocessing. All processing was performed directly in the RGB color space, which worked reasonably well for small amplification factors but quickly led to color distortions when amplification increased.\n\nMagnification was applied uniformly over the entire frame, which meant that background regions and irrelevant motion were amplified together with the signal of interest. A Laplacian pyramid was used, but with a fixed number of levels and without any adaptive strategy to control amplification across spatial scales. Temporal filtering was also fairly naive, relying on basic bandpass filtering with limited control over phase behavior.\n\nWhile this version was useful for understanding the core mechanism, it became clear during testing that it was fragile and prone to noise and artifacts.\n\n---\n\n### Expanded, Paper-Inspired Version\n\nThe refined implementation incorporates several design choices that are directly motivated by Wu et al. (2012).\n\nInstead of working purely in RGB, the pipeline converts frames to the YIQ color space, separating luminance from chrominance. This change alone significantly improves color stability when amplifying subtle signals, as chrominance channels can be amplified without distorting overall brightness.\n\nFace detection using Haar Cascades was introduced to restrict amplification to the facial region. This decision was driven by practical observation: amplifying the entire frame often emphasized background noise more than the physiological signal itself. By focusing only on the face, the signal-to-noise ratio improves noticeably.\n\nSpatial processing was also refined through a multiscale Laplacian pyramid. Rather than treating all spatial frequencies equally, each pyramid level is filtered and amplified independently, following the coarse-to-fine strategy described in the paper. Temporal filtering was upgraded to a zero-phase Butterworth bandpass filter, which provides more precise frequency isolation and avoids phase distortions that were visible in earlier experiments.\n\nFinally, amplification is no longer uniform across pyramid levels. Instead, it is attenuated at higher spatial frequencies according to the paper’s formulation, which helps prevent the introduction of high-frequency artifacts during reconstruction.\n\n---\n\n## How the Pipeline Evolved, Step by Step\n\nRather than a complete rewrite, the implementation evolved incrementally:\n\n- **Imports and setup** expanded to include scientific filtering tools and more robust error handling.\n- **Video I/O** was improved with consistent normalization, shape checks, and flexible output codecs.\n- **Color space handling** was added to decouple luminance and chrominance and reduce clipping.\n- **Face masking** was introduced to localize amplification and suppress background flicker.\n- **Laplacian pyramids** were extended to multiple levels, enabling scale-specific processing.\n- **Temporal filtering** moved from simple bandpass logic to zero-phase Butterworth filters.\n- **Mask resizing** ensured that the region of interest remained consistent across pyramid levels.\n- **Adaptive amplification** scaled the amplification factor by spatial level, following the paper’s derivation.\n\nEach of these changes was motivated by issues observed during testing rather than by theoretical completeness alone.\n\n---\n\n## Key Concepts Borrowed from the Paper\n\nSeveral ideas from Wu et al. (2012) proved particularly important in practice. The first-order motion approximation explains why small variations can be magnified without explicit optical flow. Careful frequency range selection makes it possible to target specific physiological signals such as heartbeat or breathing. Masking and multiscale analysis help suppress noise and avoid artifacts, especially at higher spatial frequencies.\n\nIn particular, the constraint  \n\\[\n(1 + \\alpha)\\,\\delta(t) < \\lambda / 8\n\\]  \nprovides a useful guideline for controlling amplification and was directly reflected in the adaptive scaling strategy used in the final implementation.\n\n---\n\n## Conclusion\n\nComparing the initial, minimal pipeline with the expanded, paper-aligned implementation highlights substantial improvements in visual stability, color fidelity, and robustness to noise. The refined version is not only closer to the methodology described by Wu et al. (2012), but also significantly more usable in practice.\n\nRather than simply adding complexity, each refinement addresses a specific limitation observed in earlier experiments. The result is a pipeline that produces higher-quality magnification of subtle signals such as pulse or micro-variations in color, while minimizing unintended amplification of noise or background motion.\n\n---\n\n### Reference\n\n- H.-Y. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman.  \n  *Eulerian Video Magnification for Revealing Subtle Changes in the World.*  \n  ACM Transactions on Graphics (Proc. SIGGRAPH), 2012.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 1: IMPORTS & BASIC SETUP\n# ===============================================\nimport cv2\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:20.964373Z","iopub.execute_input":"2026-01-29T15:24:20.964630Z","iopub.status.idle":"2026-01-29T15:24:22.915955Z","shell.execute_reply.started":"2026-01-29T15:24:20.964607Z","shell.execute_reply":"2026-01-29T15:24:22.915108Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**Formulas/Concepts**  \n- Loading a video involves reading each frame via OpenCV’s `VideoCapture`.\n- Saving a video uses OpenCV’s `VideoWriter` with a chosen codec (here, 'XVID').\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 2: VIDEO I/O UTILITIES\n# ===============================================\ndef load_video(path):\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"Video not found: {path}\")\n    cap = cv2.VideoCapture(path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        # Normalize to [0, 1]\n        frames.append(frame.astype(np.float32) / 255.0)\n    cap.release()\n    return np.array(frames), fps\n\ndef save_video(video, fps, filename):\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    h, w = video.shape[1:3]\n    out = cv2.VideoWriter(filename, fourcc, fps, (w, h), True)\n    for frame in video:\n        out.write((frame * 255).astype(np.uint8))\n    out.release()\n    print(f\"Saved {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.916969Z","iopub.execute_input":"2026-01-29T15:24:22.917356Z","iopub.status.idle":"2026-01-29T15:24:22.924237Z","shell.execute_reply.started":"2026-01-29T15:24:22.917322Z","shell.execute_reply":"2026-01-29T15:24:22.923400Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Step 3: Color Space Conversions (RGB ↔ YIQ)\n\nAt this point, I move away from the standard RGB color space and switch to **YIQ**.  \nThe main reason for this choice is practical rather than theoretical: when I tried amplifying signals directly in RGB, even moderate amplification values started to introduce visible color distortions, especially on skin tones.\n\nBy working in YIQ space, I can clearly separate **brightness information** from **color information**, which makes the amplification step much more stable.\n\n---\n\n### Why YIQ?\n\nIn RGB, intensity and color are tightly coupled.  \nThis means that amplifying subtle temporal changes in one channel often unintentionally alters the perceived color balance of the entire frame.\n\nYIQ addresses this by splitting the signal into:\n\n- **Y**: overall intensity (luminance)\n- **I, Q**: chrominance (color information)\n\nThis separation is particularly useful for Eulerian Video Magnification, where the goal is to amplify *very small temporal variations* (such as pulse-related intensity changes) without introducing artificial color shifts.\n\n---\n\n### RGB → YIQ Transformation\n\nThe conversion from RGB to YIQ is performed using a fixed linear transformation:\n\n\\[\n\\begin{bmatrix}\nY \\\\\nI \\\\\nQ\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.299 & 0.587 & 0.114 \\\\\n0.596 & -0.274 & -0.322 \\\\\n0.211 & -0.523 & 0.312\n\\end{bmatrix}\n\\begin{bmatrix}\nR \\\\\nG \\\\\nB\n\\end{bmatrix}\n\\]\n\nThis projection maps the RGB values into one luminance channel and two chrominance channels.  \nIn practice, this makes subtle temporal variations easier to isolate and analyze, especially when they are primarily expressed as small intensity changes.\n\n---\n\n### YIQ → RGB Conversion\n\nAfter temporal filtering and amplification, the processed video must be converted back to RGB for visualization and saving.\n\nThis is done by applying the **inverse** of the RGB→YIQ transformation matrix.  \nIn the implementation, the inverse is computed numerically using `np.linalg.inv` to ensure accuracy and consistency.\n\n---\n\n### Practical Insight\n\nIn practice, operating in YIQ space significantly reduces color artifacts compared to working directly in RGB.  \nI observed that amplified signals remain clearly visible, while skin tones and overall color appearance stay natural and stable.\n\nThis step turned out to be essential for making Eulerian Video Magnification robust, especially when targeting physiological signals such as heartbeat-related color fluctuations.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 3: COLOR SPACE CONVERSIONS (RGB <-> YIQ)\n# ===============================================\ndef rgb_to_yiq(img):\n    transform = np.array([[0.299, 0.587, 0.114],\n                          [0.596, -0.274, -0.322],\n                          [0.211, -0.523, 0.312]])\n    return np.dot(img, transform.T)\n\ndef yiq_to_rgb(img):\n    transform = np.linalg.inv(np.array([[0.299, 0.587, 0.114],\n                                        [0.596, -0.274, -0.322],\n                                        [0.211, -0.523, 0.312]]))\n    return np.dot(img, transform.T)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.926069Z","iopub.execute_input":"2026-01-29T15:24:22.926353Z","iopub.status.idle":"2026-01-29T15:24:22.943847Z","shell.execute_reply.started":"2026-01-29T15:24:22.926322Z","shell.execute_reply":"2026-01-29T15:24:22.943202Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Step 4: Face Mask Generation (Haar Cascades)\n\nAt this stage, I restrict the amplification process to the **facial region only**.  \nThe motivation behind this step is simple: applying Eulerian magnification to the entire frame often amplifies background noise and irrelevant motion, which can easily overwhelm the subtle physiological signals of interest.\n\nTo address this, I use OpenCV’s **Haar Cascade face detector** to automatically locate the face and generate a **binary mask**.\n\n---\n\n### What the Mask Represents\n\nThe generated mask follows a very simple rule:\n\n- Pixels **inside** the detected face region are set to **1**\n- Pixels **outside** the face region are set to **0**\n\nThis allows all subsequent amplification steps to focus exclusively on the facial area, while leaving the background untouched.\n\n---\n\n### Why Haar Cascades?\n\nHaar Cascades are a classical but reliable object detection method based on:\n- hand-crafted features,\n- a cascade of trained classifiers,\n- and fast evaluation on grayscale images.\n\nAlthough more modern face detectors exist, Haar Cascades are:\n- lightweight,\n- easy to integrate,\n- and sufficiently robust for this controlled experimental setup.\n\nThe function `detectMultiScale` returns bounding boxes corresponding to detected faces in the frame, which are then directly converted into a binary spatial mask.\n\n---\n\n### Practical Note\n\nIn practice, even a coarse face mask significantly improves the stability of Eulerian Video Magnification.  \nBy limiting amplification to the face, background flickering and noise are largely suppressed, while pulse-related color variations become more visible and interpretable.\n\nThis masking step plays a key role in making the overall pipeline more robust and closer to the methodology described in the original Eulerian Video Magnification paper.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 4: FACE MASK GENERATION (Haar Cascades)\n# ===============================================\ndef generate_face_mask(frame):\n    gray = (frame[:, :, 0] * 255).astype(np.uint8)\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    mask = np.zeros_like(gray, dtype=np.float32)\n    for (x, y, w, h) in faces:\n        mask[y:y + h, x:x + w] = 1.0\n    return np.expand_dims(mask, axis=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.944826Z","iopub.execute_input":"2026-01-29T15:24:22.945287Z","iopub.status.idle":"2026-01-29T15:24:22.957872Z","shell.execute_reply.started":"2026-01-29T15:24:22.945256Z","shell.execute_reply":"2026-01-29T15:24:22.957144Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Step 5: Laplacian Pyramid Construction\n\nAt this stage, I decompose each frame into a **Laplacian pyramid** in order to separate image information across different spatial scales. The motivation here is simple: subtle physiological signals, such as tiny facial color variations, tend to appear at specific spatial frequencies. Amplifying the entire image uniformly would only increase noise and visual artifacts.\n\nBy working with a coarse-to-fine representation, I gain much better control over which spatial details are later amplified and which ones are left untouched.\n\n---\n\n### Why a Laplacian Pyramid?\n\nIn Eulerian Video Magnification, applying amplification directly in the pixel domain often leads to unstable results. Using a Laplacian pyramid helps because:\n\n- Fine details and smooth structures are separated across different levels.\n- Noise amplification can be limited by controlling the contribution of high-frequency levels.\n- Relevant physiological signals can be isolated more cleanly.\n\nIn practice, this makes the amplification process more stable and visually realistic.\n\n---\n\n### Building the Pyramid\n\nFor each pyramid level, the following steps are applied:\n\n1. The current image is **downsampled** using `pyrDown`.\n2. The downsampled image is **upsampled back** using `pyrUp`.\n3. The difference between the original image and its upsampled version forms a **Laplacian level**, capturing details at that spatial scale.\n\nFormally, this can be written as:\n\n\\[\nL_i = I_i - \\text{pyrUp}(\\text{pyrDown}(I_i))\n\\]\n\nwhere \\(I_i\\) denotes the image at level \\(i\\), and \\(L_i\\) contains the spatial details lost during downsampling.\n\nThe final downsampled image is stored as the coarsest level of the pyramid.\n\n---\n\n### Pyramid Reconstruction\n\nAfter temporal filtering and amplification, the image is reconstructed by starting from the coarsest level and iteratively upsampling and adding back the stored Laplacian details. This reconstruction step preserves the global structure of the frame while reintroducing the amplified fine-scale information.\n\n---\n\n### Key Takeaways\n\n- **Multiscale representation** allows selective amplification at different spatial frequencies.\n- **Smooth reconstruction** using `pyrDown` and `pyrUp` avoids visible seams or distortions.\n- **Better control** over noise and artifacts compared to single-scale amplification.\n\nThis step plays a crucial role in making Eulerian Video Magnification reliable when applied to real-world videos, especially when targeting subtle facial signals rather than large motions.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 5: LAPLACIAN PYRAMID CONSTRUCTION\n# ===============================================\ndef build_laplacian_pyramid(frame, levels=5):\n    pyramid = []\n    current = frame\n    for _ in range(levels):\n        down = cv2.pyrDown(current)\n        up = cv2.pyrUp(down, dstsize=(current.shape[1], current.shape[0]))\n        lap = current - up\n        pyramid.append(lap)\n        current = down\n    pyramid.append(current)\n    return pyramid\n\ndef collapse_laplacian_pyramid(pyramid):\n    output = pyramid[-1]\n    for lvl in reversed(range(len(pyramid) - 1)):\n        up = cv2.pyrUp(output, dstsize=(pyramid[lvl].shape[1], pyramid[lvl].shape[0]))\n        output = pyramid[lvl] + up\n    return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.958827Z","iopub.execute_input":"2026-01-29T15:24:22.959127Z","iopub.status.idle":"2026-01-29T15:24:22.977931Z","shell.execute_reply.started":"2026-01-29T15:24:22.959094Z","shell.execute_reply":"2026-01-29T15:24:22.977183Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Step 6: Temporal Butterworth Bandpass Filter\n\nAt this stage, the focus shifts from *space* to *time*.  \nInstead of asking *where* changes happen in the image, I ask **which temporal patterns persist over time** and whether they match the expected physiological frequencies.\n\nThe idea is simple: if a signal is caused by a heartbeat or blood flow, it should oscillate within a very narrow and predictable frequency range. Everything outside that range is likely noise, lighting fluctuation, or unrelated motion.\n\n---\n\n### Why Temporal Filtering?\n\nIn raw video data, each pixel changes over time for many reasons:\n- illumination variations,\n- small camera movements,\n- sensor noise,\n- and, occasionally, real physiological signals.\n\nDirect amplification would boost **all of them**, which is rarely what we want.  \nTemporal bandpass filtering allows me to **isolate only the frequency band of interest** before amplification.\n\nFor this project, the target range is chosen based on typical physiological rates (e.g. heart rate), while suppressing slower trends and high-frequency noise.\n\n---\n\n### Butterworth Filter: Practical Choice\n\nI use a **Butterworth bandpass filter** because of its smooth frequency response.  \nUnlike sharper filters, Butterworth filters avoid strong ripples in the passband, which helps keep the amplified signal visually stable.\n\nAnother important design choice is applying the filter with `filtfilt` instead of a single forward pass.  \nThis performs **zero-phase filtering**, meaning:\n\n- no temporal delay is introduced,\n- rising and falling patterns remain aligned in time,\n- the amplified signal looks natural rather than “shifted”.\n\nThis matters a lot when visualizing periodic biological signals.\n\n---\n\n### Continuous vs. Discrete Perspective (Intuition Only)\n\nIn theory, a Butterworth bandpass filter can be seen as the combination of:\n- a low-pass filter (removing fast fluctuations),\n- and a high-pass filter (removing slow trends).\n\nIn practice, I do **not** work with the continuous formula directly.  \nInstead, cutoff frequencies are normalized relative to the **Nyquist frequency** (half the frame rate), which is standard in discrete signal processing.\n\n---\n\n### Implementation Insight\n\nThe filtering is applied **along the temporal axis** for each pixel independently.  \nTo make this efficient, the video tensor is reshaped so that time becomes the leading dimension, the filter is applied once, and the original shape is restored afterward.\n\nThis approach keeps the implementation simple while remaining faithful to the theoretical model described in the original Eulerian Video Magnification paper.\n\nOverall, this step is crucial: without proper temporal filtering, amplification quickly becomes unstable and visually unconvincing.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 6: TEMPORAL BUTTERWORTH BANDPASS FILTER\n# ===============================================\ndef butter_bandpass_filter(data, fps, freq_min, freq_max, order=3):\n    nyquist = 0.5 * fps\n    low, high = freq_min / nyquist, freq_max / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    original_shape = data.shape\n    reshaped = data.reshape((original_shape[0], -1))\n    filtered = filtfilt(b, a, reshaped, axis=0)\n    return filtered.reshape(original_shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.978794Z","iopub.execute_input":"2026-01-29T15:24:22.979121Z","iopub.status.idle":"2026-01-29T15:24:22.992401Z","shell.execute_reply.started":"2026-01-29T15:24:22.979094Z","shell.execute_reply":"2026-01-29T15:24:22.991623Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Step 7: Face Mask Generation (Resized per Pyramid Level)\n\nAt this point, I refine the face mask generation to make sure that the amplification is applied **only where it makes sense**.  \nInstead of magnifying the entire frame, I explicitly restrict the process to the facial region, which is where the physiological color changes of interest are expected to appear.\n\nA key issue here is that the video is processed at multiple spatial scales due to the Laplacian pyramid.  \nIf the face mask is not resized accordingly at each level, the region of interest becomes misaligned, leading to artifacts or partial amplification of background areas.\n\n---\n\n### Practical Idea Behind the Mask\n\nThe approach follows a simple logic:\n\n1. **Detect the face once per frame** using a classical Haar Cascade detector.\n2. **Create a binary mask**:\n   - pixels inside the detected face region are set to 1,\n   - pixels outside are set to 0.\n3. **Resize the mask** to match each pyramid level before applying amplification.\n\nBy doing this, the magnification remains spatially consistent across all pyramid levels.\n\n---\n\n### Why This Step Matters\n\nWithout a properly resized face mask:\n- background regions can start to flicker after amplification,\n- noise outside the face may dominate the signal,\n- and subtle color changes on the face become harder to interpret.\n\nRestricting the amplification to the facial area significantly improves both **visual quality** and **signal-to-noise ratio**, especially when targeting weak physiological signals such as pulse-related color variations.\n\nThis step acts as a spatial constraint that complements the temporal filtering applied earlier.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 7: FACE MASK GENERATION (Resized per level)\n# ===============================================\ndef generate_face_mask(frame):\n    \"\"\"\n    Detects the face in the frame using Haar cascades and returns a binary mask.\n    \"\"\"\n    gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n    \n    mask = np.zeros_like(gray, dtype=np.float32)\n    for (x, y, w, h) in faces:\n        mask[y:y + h, x:x + w] = 1.0\n    return mask[..., np.newaxis]  # Add channel dimension\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:22.994071Z","iopub.execute_input":"2026-01-29T15:24:22.994347Z","iopub.status.idle":"2026-01-29T15:24:23.007981Z","shell.execute_reply.started":"2026-01-29T15:24:22.994319Z","shell.execute_reply":"2026-01-29T15:24:23.007079Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Step 8: Multiscale Eulerian Video Magnification (With Resizing)\n\n### Purpose  \nIn this final step, all previous components are combined into a **complete Eulerian Video Magnification (EVM) pipeline**.  \nThe goal is to amplify **subtle temporal variations** in the video, such as physiological signals (e.g. heartbeat), while preserving visual realism and avoiding noise or color artifacts.\n\nRather than amplifying raw RGB values directly, this pipeline relies on **color space separation**, **multiscale spatial decomposition**, and **temporal filtering**, closely following the principles described in the original EVM framework.\n\n---\n\n### Pipeline Overview  \n\nThe processing pipeline consists of the following steps:\n\n1. **Load the input video** and extract frames along with the frame rate.  \n2. **Convert frames to YIQ color space**, separating luminance (Y) from chrominance (I, Q) to improve color stability during amplification.  \n3. **Detect the face region** in the first frame and generate a binary mask to restrict amplification to relevant areas.  \n4. **Build a Laplacian pyramid** for each frame, decomposing it into multiple spatial frequency bands.  \n5. **Resize and apply the face mask** at each pyramid level to ensure spatial alignment across scales.  \n6. **Apply a temporal Butterworth bandpass filter** at each pyramid level to isolate the target frequency band.  \n7. **Amplify the filtered signals**, using a scale-dependent amplification factor to reduce noise at finer spatial levels.  \n8. **Collapse the Laplacian pyramid** to reconstruct the amplified frames.  \n9. **Convert frames back to RGB** and save the final magnified video.\n\nThis multiscale approach allows subtle signals to be enhanced while minimizing artifacts and background noise.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Key Equations and Intuition\n\n### Eulerian Video Magnification: Core Idea\n\nAt the heart of Eulerian Video Magnification, the goal is to enhance **very small temporal variations** in pixel intensity, without explicitly tracking motion or computing optical flow.\n\nConceptually, the magnified signal can be written as:\n\n$$\nI'(x, t) = I(x, t) + \\alpha \\cdot \\text{filtered}\\bigl(I(x, t)\\bigr)\n$$\n\nwhere:\n\n- \\( I(x, t) \\) is the original pixel intensity at spatial location \\(x\\) and time \\(t\\),\n- \\( \\text{filtered}\\bigl(I(x, t)\\bigr) \\) is the temporally band-pass filtered version of the signal, isolating the frequency range of interest (e.g. heartbeat),\n- \\( \\alpha \\) is the amplification factor controlling how strongly the filtered signal is emphasized.\n\nInstead of following motion trajectories, the Eulerian approach works directly at **fixed pixel locations**, amplifying subtle temporal changes that are otherwise invisible.\n\n---\n\n### Scale-Dependent (Spatially Varying) Amplification\n\nApplying the same amplification at all spatial scales tends to amplify noise, especially at fine details.  \nTo avoid this, the amplification factor is adapted across the levels of the Laplacian pyramid:\n\n$$\n\\alpha_{\\text{level}} =\n\\frac{\\alpha}{2^{(\\text{pyramid\\_levels} - \\text{level} - 1)}}\n$$\n\nIn practice:\n\n- Coarser pyramid levels (large spatial structures) receive **stronger amplification**.\n- Finer levels (small details) are amplified **less aggressively**.\n\nThis strategy helps suppress high-frequency noise and visual artifacts, while still revealing meaningful large-scale variations such as subtle head motion, breathing, or pulse-related color changes.\n\n---\n\n### Practical Insight\n\nUsing multiscale decomposition together with scale-dependent amplification produces noticeably cleaner results than uniform amplification.  \nSubtle physiological signals remain visible, while skin tones and overall color appearance stay natural.\n\nThis balance is essential for building a **stable and robust Eulerian Video Magnification pipeline**, especially when targeting weak signals such as pulse-induced color variations.\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 8: MULTISCALE EULERIAN VIDEO MAGNIFICATION (With Resizing)\n# ===============================================\ndef eulerian_video_magnification(\n    input_path,\n    output_path,\n    freq_min=0.8,\n    freq_max=1.0,\n    amplification=50,\n    pyramid_levels=5\n):\n    # 1) Load video\n    video, fps = load_video(input_path)\n    nframes, h, w, c = video.shape\n    \n    # 2) Convert to YIQ for color fidelity\n    yiq_video = np.array([rgb_to_yiq(frame) for frame in video], dtype=np.float32)\n\n    # 3) Generate face mask from the first frame\n    original_face_mask = generate_face_mask(video[0])\n\n    # 4) Build Laplacian pyramid for each frame, store timeseries\n    pyramid_timeseries = [[] for _ in range(pyramid_levels + 1)]\n    for i in range(nframes):\n        pyr = build_laplacian_pyramid(yiq_video[i], levels=pyramid_levels)\n        for level_idx in range(pyramid_levels + 1):\n            # Resize face mask to match pyramid level dimensions\n            resized_mask = cv2.resize(original_face_mask, (pyr[level_idx].shape[1], pyr[level_idx].shape[0]))\n            resized_mask = resized_mask[..., np.newaxis]  # Ensure channel dimension\n            pyramid_timeseries[level_idx].append(pyr[level_idx] * resized_mask)\n\n    for level_idx in range(pyramid_levels + 1):\n        pyramid_timeseries[level_idx] = np.stack(pyramid_timeseries[level_idx], axis=0)\n\n    # 5) Apply temporal Butterworth bandpass filter + amplification\n    for level_idx in range(pyramid_levels + 1):\n        # Decrease amplification for higher spatial frequencies\n        alpha = amplification / (2 ** (pyramid_levels - level_idx - 1))\n        filtered = butter_bandpass_filter(pyramid_timeseries[level_idx], fps, freq_min, freq_max, order=3)\n        pyramid_timeseries[level_idx] += filtered * alpha\n\n    # 6) Reconstruct frames by collapsing the pyramid and converting back to RGB\n    out_frames = []\n    for i in range(nframes):\n        recon_levels = [pyramid_timeseries[level_idx][i] for level_idx in range(pyramid_levels + 1)]\n        recon_frame = collapse_laplacian_pyramid(recon_levels)\n        out_frames.append(np.clip(yiq_to_rgb(recon_frame), 0, 1))\n\n    out_frames = np.array(out_frames, dtype=np.float32)\n\n    # 7) Save the magnified video\n    save_video(out_frames, fps, output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:24:26.713710Z","iopub.execute_input":"2026-01-29T15:24:26.714527Z","iopub.status.idle":"2026-01-29T15:24:26.724774Z","shell.execute_reply.started":"2026-01-29T15:24:26.714497Z","shell.execute_reply":"2026-01-29T15:24:26.723923Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Usage Example\nAfter defining all the functions above, you can run the final pipeline by calling:","metadata":{}},{"cell_type":"code","source":"eulerian_video_magnification(\n    input_path=\"/kaggle/input/cv-eulerian-videos/face.mp4\",\n    output_path=\"magnified_face_final_optimized.avi\",\n    freq_min=0.8,\n    freq_max=1.0,\n    amplification=50,\n    pyramid_levels=5\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:28:12.271591Z","iopub.execute_input":"2026-01-29T15:28:12.272386Z","iopub.status.idle":"2026-01-29T15:28:51.715237Z","shell.execute_reply.started":"2026-01-29T15:28:12.272356Z","shell.execute_reply":"2026-01-29T15:28:51.714547Z"}},"outputs":[{"name":"stdout","text":"Saved magnified_face_final_optimized.avi\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"magnified_face_final_optimized.avi\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T15:28:51.716734Z","iopub.execute_input":"2026-01-29T15:28:51.717006Z","iopub.status.idle":"2026-01-29T15:28:51.722553Z","shell.execute_reply.started":"2026-01-29T15:28:51.716982Z","shell.execute_reply":"2026-01-29T15:28:51.721853Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/magnified_face_final_optimized.avi","text/html":"<a href='magnified_face_final_optimized.avi' target='_blank'>magnified_face_final_optimized.avi</a><br>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Final Analysis of Eulerian Video Magnification Results\n\n## Overview\nIn this final experiment, I applied the Eulerian Video Magnification (EVM) pipeline to a facial video with the aim of revealing very subtle temporal variations, mainly those linked to physiological activity such as pulse. By directly comparing the original video with the magnified output, it becomes clear that the method is able to make faint color fluctuations in the facial region visible—changes that are almost impossible to notice in the raw footage.\n\nOverall, the pipeline manages to extract meaningful temporal signals while still keeping the video visually realistic, which was one of the main constraints throughout the implementation.\n\n---\n\n## Key Observations\n\n### 1. Amplification of Subtle Color Variations\nIn the magnified video, periodic color changes become visible, especially around the forehead and cheek areas. These variations are consistent with what would be expected from blood flow–related pulsation, which suggests that the chosen frequency band (0.8–1.0 Hz) is well aligned with heartbeat dynamics.\n\nAn important point is that the amplification remains subtle. The color changes are noticeable when compared to the original video, but they do not introduce strong flickering or unnatural color shifts, which indicates that the amplification factor is reasonably balanced.\n\n### 2. Presence of Black Borders in the Output\nOne visible artifact in the final output is the presence of thin black borders around the frame. This points to a slight spatial mismatch during the reconstruction stage, most likely introduced during the Laplacian pyramid collapse or the resizing steps when using `cv2.pyrUp`.\n\nWhile this issue does not affect the temporal signal itself, it slightly reduces the visual cleanliness of the output and highlights the importance of careful spatial alignment in multiscale reconstruction.\n\n### 3. Face Mask Localization\nThe face mask plays a key role in restricting amplification to the facial region. In practice, it successfully prevents most of the background from being amplified, which significantly improves the clarity of the result.\n\nThat said, a small amount of amplification leakage can still be observed near the boundaries of the mask. This suggests that the mask could be refined further, but even in its current form, it represents a clear improvement over applying uniform amplification to the entire frame.\n\n### 4. Temporal Stability\nThe amplified signal evolves smoothly over time, without abrupt jumps or visible temporal jitter. This indicates that the Butterworth bandpass filter is operating as intended and that the target frequency range is being isolated consistently.\n\nFrom a qualitative point of view, this temporal stability increases confidence that the observed color variations correspond to real underlying signals rather than filtering artifacts.\n\n### 5. Noise and Over-Amplification Effects\nIn a few localized regions—particularly around sharp edges such as glasses or high-contrast background contours—slight over-amplification can be observed. These effects are most likely introduced at higher pyramid levels, where fine spatial details are more prominent but less relevant to the physiological signal of interest.\n\nHowever, these artifacts remain limited in scope and do not dominate the overall visual impression of the video.\n\n---\n\n## Suggestions for Improvement\nAlthough no further modifications were applied due to time constraints, several improvements could reasonably be explored:\n\n1. **Correct Frame Size Mismatch**  \n   Ensuring consistent spatial dimensions during pyramid reconstruction, especially when using `cv2.pyrUp`, would likely remove the black borders observed in the output.\n\n2. **Refine the Face Mask Strategy**  \n   Using frame-by-frame face detection or simple tracking instead of relying on the first frame only could improve mask accuracy in dynamic scenes. Applying basic morphological operations could also help smooth mask boundaries.\n\n3. **Limit High-Frequency Amplification**  \n   Further reducing the amplification factor at higher pyramid levels, or decreasing the total number of pyramid levels, could help suppress edge-related noise.\n\n4. **Adapt the Frequency Range to the Target Signal**  \n   If the focus shifts from pulse to slower physiological processes such as breathing, lowering the frequency band (e.g., 0.2–0.5 Hz) would be more appropriate.\n\n---\n\n## Conclusion\nThe final EVM pipeline succeeds in revealing subtle physiological color variations in facial video data while maintaining temporal stability and overall visual realism. Although minor artifacts such as black borders and localized over-amplification are present, they do not undermine the main objective of the experiment.\n\nOverall, this implementation shows that even a relatively simple, paper-inspired Eulerian Video Magnification pipeline can effectively expose weak temporal signals. With modest refinements in spatial alignment, masking, and scale-dependent amplification, the quality and robustness of the results could be improved further.\n","metadata":{}}]}