{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10842744,"sourceType":"datasetVersion","datasetId":6733723}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Negin Heidarifard**  \n**M2 in Artificial Intelligence, Paris-Saclay University**  \n**Course: Computer Vision**  \n**Professor: Dr. Celine Hudelot** 🎓\n\n---\n\n## **Project Introduction** 💡\n\nThis project showcases **Eulerian Video Magnification** (Wu et al., 2012) to unveil **subtle spatiotemporal variations** in video. By isolating and amplifying specific frequency bands, we can reveal minute color changes (such as blood flow in the face) or micro-motions that are usually **invisible** to the naked eye. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:02.323836Z","iopub.execute_input":"2025-02-25T22:13:02.324254Z","iopub.status.idle":"2025-02-25T22:13:02.642174Z","shell.execute_reply.started":"2025-02-25T22:13:02.324211Z","shell.execute_reply":"2025-02-25T22:13:02.641373Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/final-cv-dataset/wrist.mp4\n/kaggle/input/final-cv-dataset/face.mp4\n/kaggle/input/final-cv-dataset/baby.mp4\n/kaggle/input/final-cv-dataset/baby2.mp4\n/kaggle/input/final-cv-dataset/face3.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from IPython.display import IFrame\nIFrame('https://www.youtube.com/embed/ONZcjs1Pjmk', width=700, height=350)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:02.643501Z","iopub.execute_input":"2025-02-25T22:13:02.643979Z","iopub.status.idle":"2025-02-25T22:13:02.649862Z","shell.execute_reply.started":"2025-02-25T22:13:02.643944Z","shell.execute_reply":"2025-02-25T22:13:02.649080Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7bf5f7f781c0>","text/html":"\n        <iframe\n            width=\"700\"\n            height=\"350\"\n            src=\"https://www.youtube.com/embed/ONZcjs1Pjmk\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Final Project: Eulerian Video Magnification for Subtle Color Changes  \n**Course:** Introduction to Visual Computing  \n## **Team Members:** *Negin HEIDARIFARD*  \n\n## Introduction  \nIn this project, we implement Eulerian Video Magnification (EVM) to reveal subtle color changes in video data. The main idea is to decompose a video spatially (using a Laplacian pyramid), apply temporal bandpass filtering to extract specific frequency bands, and amplify these signals to make imperceptible changes visible. This technique is particularly useful for detecting the pulse in a face (via slight color variations), among other applications.\n\nIn the following cells, we present the implementation step by step, along with detailed explanations for each component.\n","metadata":{}},{"cell_type":"code","source":"# Cell 2: Imports & Environment Setup\nimport cv2\nimport numpy as np\nimport scipy.fftpack\nimport scipy.signal\nimport os\nimport gc\nfrom IPython.display import FileLink, display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:02.651251Z","iopub.execute_input":"2025-02-25T22:13:02.651496Z","iopub.status.idle":"2025-02-25T22:13:03.482965Z","shell.execute_reply.started":"2025-02-25T22:13:02.651477Z","shell.execute_reply":"2025-02-25T22:13:03.482282Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Explanation:**  \nWe import the required libraries:  \n- **cv2** for image/video processing  \n- **numpy** for array manipulation  \n- **scipy.fftpack and scipy.signal** for FFT-based filtering  \n- **os, gc** for file handling and memory management  \n- **IPython.display** for creating download links.  \nThis cell sets up our working environment.\n","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions\n**Explanation:**  \nThese functions handle loading a video (without downsampling) and saving processed videos as AVI files.  \n- **load_video_no_downsample()** reads the video frame by frame and converts the pixel values to the [0,1] range.  \n- **save_video_float32_as_avi()** converts the processed float32 video back to uint8 and writes it using the XVID codec.\n","metadata":{}},{"cell_type":"code","source":"# Cell 3: Helper Functions\n\ndef load_video_no_downsample(video_filename):\n    \"\"\"\n    Loads the entire video at its original resolution as a float32 array (values in [0,1]).\n    Use with caution on large videos.\n    \"\"\"\n    if not os.path.isfile(video_filename):\n        raise FileNotFoundError(f\"Video not found: {video_filename}\")\n    cap = cv2.VideoCapture(video_filename)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = []\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_f = frame_bgr.astype(np.float32) / 255.0\n        frames.append(frame_f)\n    cap.release()\n    video_array = np.array(frames, dtype=np.float32)\n    return video_array, fps\n\ndef save_video_float32_as_avi(video_data, fps, out_filename=\"output.avi\"):\n    \"\"\"\n    Saves a float32 video (values in [0,1]) as an AVI file using the XVID codec.\n    \"\"\"\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    h, w = video_data.shape[1:3]\n    out = cv2.VideoWriter(out_filename, fourcc, fps, (w, h), True)\n    for i in range(video_data.shape[0]):\n        frame_uint8 = np.clip(video_data[i] * 255.0, 0, 255).astype(np.uint8)\n        out.write(frame_uint8)\n    out.release()\n    print(f\"Saved {out_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:03.484102Z","iopub.execute_input":"2025-02-25T22:13:03.484547Z","iopub.status.idle":"2025-02-25T22:13:03.490783Z","shell.execute_reply.started":"2025-02-25T22:13:03.484514Z","shell.execute_reply":"2025-02-25T22:13:03.489929Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Eulerian Motion Magnification Functions\n\n**Explanation:**  \nThis cell defines the core functions for the Eulerian Video Magnification process:  \n- **create_laplacian_pyramid_frame()** and **create_laplacian_video_pyramid()** decompose each frame (and the whole video) into a Laplacian pyramid.  \n- **collapse_laplacian_pyramid_frame()** and **collapse_laplacian_video_pyramid()** reconstruct the frame (or video) from the pyramid.  \n- **temporal_bandpass_filter()** applies an FFT-based bandpass filter on the video over time.  \n- **eulerian_magnification()** ties these steps together by processing each pyramid level (except the top and bottom) and then reconstructing the magnified video.\n","metadata":{}},{"cell_type":"code","source":"# Cell 4: Eulerian Motion Magnification Functions\n\ndef create_laplacian_pyramid_frame(frame, pyramid_levels=4):\n    gauss_pyr = [frame]\n    for _ in range(1, pyramid_levels):\n        gauss_pyr.append(cv2.pyrDown(gauss_pyr[-1]))\n    lap_pyr = []\n    for i in range(pyramid_levels - 1):\n        up = cv2.pyrUp(gauss_pyr[i+1])\n        h, w = gauss_pyr[i].shape[:2]\n        up = up[:h, :w]\n        lap_pyr.append(gauss_pyr[i] - up)\n    lap_pyr.append(gauss_pyr[-1])\n    return lap_pyr\n\ndef create_laplacian_video_pyramid(video, pyramid_levels=4):\n    nframes = video.shape[0]\n    pyramid = None\n    for i in range(nframes):\n        frame_pyr = create_laplacian_pyramid_frame(video[i], pyramid_levels)\n        if pyramid is None:\n            pyramid = []\n            for lvl in range(pyramid_levels):\n                lvl_h, lvl_w = frame_pyr[lvl].shape[:2]\n                pyramid.append(np.zeros((nframes, lvl_h, lvl_w, 3), dtype=np.float32))\n        for lvl in range(pyramid_levels):\n            pyramid[lvl][i] = frame_pyr[lvl]\n    return pyramid\n\ndef collapse_laplacian_pyramid_frame(lap_pyr):\n    output = lap_pyr[-1]\n    for lvl in reversed(range(len(lap_pyr) - 1)):\n        up = cv2.pyrUp(output)\n        h, w = lap_pyr[lvl].shape[:2]\n        up = up[:h, :w]\n        output = lap_pyr[lvl] + up\n    return output\n\ndef collapse_laplacian_video_pyramid(pyramid):\n    nframes = pyramid[0].shape[0]\n    collapsed_frames = []\n    for i in range(nframes):\n        lap_pyr_frame = [pyramid[lvl][i] for lvl in range(len(pyramid))]\n        collapsed_frame = collapse_laplacian_pyramid_frame(lap_pyr_frame)\n        collapsed_frames.append(collapsed_frame)\n    return np.array(collapsed_frames, dtype=np.float32)\n\ndef temporal_bandpass_filter(data, fps, freq_min, freq_max, amplification=1.0, axis=0):\n    fft_data = scipy.fftpack.rfft(data, axis=axis)\n    freqs = scipy.fftpack.rfftfreq(data.shape[0], d=1.0/fps)\n    low_idx = np.argmin(np.abs(freqs - freq_min))\n    high_idx = np.argmin(np.abs(freqs - freq_max))\n    fft_data[:low_idx] = 0\n    fft_data[high_idx+1:] = 0\n    filtered = scipy.fftpack.irfft(fft_data, axis=axis)\n    filtered *= amplification\n    return filtered\n\ndef eulerian_magnification(vid_data, fps, freq_min, freq_max, amplification,\n                           pyramid_levels=4, skip_levels_at_top=1):\n    vid_pyr = create_laplacian_video_pyramid(vid_data, pyramid_levels)\n    for lvl in range(len(vid_pyr)):\n        if lvl < skip_levels_at_top or lvl == len(vid_pyr) - 1:\n            continue\n        bandpassed = temporal_bandpass_filter(vid_pyr[lvl], fps, freq_min, freq_max, amplification, axis=0)\n        vid_pyr[lvl] += bandpassed\n    return collapse_laplacian_video_pyramid(vid_pyr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:03.491594Z","iopub.execute_input":"2025-02-25T22:13:03.491855Z","iopub.status.idle":"2025-02-25T22:13:03.514733Z","shell.execute_reply.started":"2025-02-25T22:13:03.491836Z","shell.execute_reply":"2025-02-25T22:13:03.513925Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Eulerian Color Amplification Function\n**Explanation:**  \nThis function is a variant of the Eulerian magnification pipeline that focuses solely on amplifying color changes. It does not track or amplify motion. Instead, it creates a lowpass (Gaussian) representation of each frame, applies temporal filtering to extract and amplify subtle color signals, upsamples the signal back to the original resolution, and then adds it to the original frame.\n","metadata":{}},{"cell_type":"code","source":"# Cell 5: Eulerian Color Amplification (Color-Only) Function\n\ndef eulerian_color_amplification(vid_data, fps, freq_min, freq_max, amplification, pyramid_levels=4):\n    \"\"\"\n    Amplifies subtle color changes only (without amplifying motion):\n      1) Downsample each frame (pyramid_levels-1 times) to get a coarse color representation.\n      2) Apply a temporal bandpass filter on the coarse video.\n      3) Upsample each filtered frame back to the original resolution.\n      4) Add the upsampled, amplified color signal to the original frame.\n    \"\"\"\n    nframes, orig_h, orig_w, _ = vid_data.shape\n    gauss_frames = []\n    for i in range(nframes):\n        frame = vid_data[i]\n        for _ in range(pyramid_levels - 1):\n            frame = cv2.pyrDown(frame)\n        gauss_frames.append(frame)\n    gauss_video = np.array(gauss_frames, dtype=np.float32)\n    bandpassed = temporal_bandpass_filter(gauss_video, fps, freq_min, freq_max, amplification, axis=0)\n    filtered_coarse = gauss_video + bandpassed\n    up_frames = []\n    for i in range(nframes):\n        up_frame = filtered_coarse[i]\n        for _ in range(pyramid_levels - 1):\n            up_frame = cv2.pyrUp(up_frame)\n        up_frame = up_frame[:orig_h, :orig_w]\n        amplified_frame = vid_data[i] + up_frame\n        amplified_frame = np.clip(amplified_frame, 0, 1)\n        up_frames.append(amplified_frame)\n    return np.array(up_frames, dtype=np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:03.515564Z","iopub.execute_input":"2025-02-25T22:13:03.515860Z","iopub.status.idle":"2025-02-25T22:13:03.533213Z","shell.execute_reply.started":"2025-02-25T22:13:03.515832Z","shell.execute_reply":"2025-02-25T22:13:03.532654Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## grid search\n\n## **Why We Need This Grid Search Utility**\n\nBefore applying Eulerian Video Magnification to a face video (or any other subtle signal detection task), we often need to **fine-tune** parameters like frequency bounds and amplification. Manually guessing these values can be inefficient and prone to error. The following two functions—**`compute_objective`** and **`grid_search_eulerian_params`**—provide a **systematic** way to find an optimal combination of parameters:\n\n1. **`compute_objective`**  \n   - **Goal**: Quantitatively measure how strongly the target signal (e.g., pulse color change) appears in the processed video.  \n   - **How**: It applies Eulerian magnification to the input video (`vid_data`) with specific frequency and amplification settings, then calculates a simple **FFT-based metric** in a chosen region of interest (ROI). This metric indicates how pronounced the signal is in the desired frequency band.\n\n2. **`grid_search_eulerian_params`**  \n   - **Goal**: Automate searching across a small range of **`freq_min`**, **`freq_max`**, and **`amplification`** values.  \n   - **How**: For each combination of parameters, it calls `compute_objective` and tracks the **best score** (i.e., the highest amplitude in the target frequency band). It then returns the parameter tuple that achieved this best score.\n\n### **Why This Matters**\n- **Eliminates Guesswork**: Instead of randomly picking frequency bounds and amplification, you get a **data-driven** approach to identify which parameters truly bring out the subtle signals in your video.  \n- **ROI Focus**: By restricting the analysis to a specific region (e.g., the face area), you ensure the algorithm focuses on the most relevant part of the frame, improving both speed and accuracy.  \n- **Robustness**: Different videos or different subjects might have slightly different optimal settings. A grid search helps you adapt to these variations quickly.  \n\nIn summary, **this grid search utility** ensures that you apply **Eulerian Magnification** in a more **targeted and quantifiable** way, maximizing the visibility of subtle color or motion signals while minimizing trial-and-error. \n","metadata":{}},{"cell_type":"code","source":"# ============================================\n# Grid Search Utility for Face Video\n# ============================================\n\ndef compute_objective(vid_data, fps, freq_min, freq_max, amplification, roi=(0,50,0,50)):\n    \"\"\"\n    Processes the video with Eulerian magnification and computes a simple objective\n    measure of how strong the color signal is within a given frequency band, measured\n    in the specified ROI.\n\n    - vid_data: the loaded video array in float32 [0..1].\n    - fps: frames per second of the video.\n    - freq_min, freq_max: frequency band for Eulerian magnification.\n    - amplification: amplification factor to test.\n    - roi: (x1, x2, y1, y2) region of interest in the frame for analyzing the signal.\n    \"\"\"\n    # You must have eulerian_magnification or eulerian_color_amplification defined above.\n    processed = eulerian_magnification(\n        vid_data,\n        fps,\n        freq_min,\n        freq_max,\n        amplification,\n        pyramid_levels=5,      # can adjust if needed\n        skip_levels_at_top=1   # or your chosen skip level\n    )\n\n    # Sum the pixel values in the ROI for each frame\n    x1, x2, y1, y2 = roi\n    roi_signal = processed[:, y1:y2, x1:x2, :].sum(axis=(1,2,3))\n\n    # Compute FFT of that signal\n    fft_vals = np.abs(np.fft.fft(roi_signal))\n    freqs = np.fft.fftfreq(len(roi_signal), d=1.0/fps)\n\n    # Only consider positive frequencies\n    pos_mask = freqs > 0\n    fft_vals = fft_vals[pos_mask]\n    freqs = freqs[pos_mask]\n\n    # Focus on a sub-band for measuring the color/motion strength\n    band_mask = (freqs >= 0.7) & (freqs <= 1.2)  # or your desired freq range for objective\n    if not np.any(band_mask):\n        return 0.0\n\n    return np.max(fft_vals[band_mask])\n\ndef grid_search_eulerian_params(vid_data, fps, roi=(0,50,0,50)):\n    \"\"\"\n    Grid-searches over a small set of freq_min, freq_max, and amplification values\n    to find the combination that yields the highest 'score' from compute_objective.\n\n    - vid_data: the loaded video array in float32 [0..1].\n    - fps: frames per second.\n    - roi: region of interest to measure the strength of the signal.\n\n    Returns: (best_params, best_score)\n    where best_params = (freq_min, freq_max, amplification)\n    and best_score is the objective measure from compute_objective.\n    \"\"\"\n    freq_min_vals = [0.7, 0.75, 0.8]\n    freq_max_vals = [1.0, 1.05, 1.1]\n    amp_vals = [70, 80, 90]\n\n    best_score = -float(\"inf\")\n    best_params = None\n\n    for fmin in freq_min_vals:\n        for fmax in freq_max_vals:\n            # Ensure freq_min < freq_max\n            if fmin >= fmax:\n                continue\n            for amp in amp_vals:\n                score = compute_objective(vid_data, fps, fmin, fmax, amp, roi)\n                if score > best_score:\n                    best_score = score\n                    best_params = (fmin, fmax, amp)\n\n    return best_params, best_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:03.534078Z","iopub.execute_input":"2025-02-25T22:13:03.534376Z","iopub.status.idle":"2025-02-25T22:13:03.549572Z","shell.execute_reply.started":"2025-02-25T22:13:03.534347Z","shell.execute_reply":"2025-02-25T22:13:03.548908Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Testing & Results \n\n**Explanation:**  \nThis cell tests the model on four videos: baby, baby2, face, and wrist. For videos where we want to detect only color changes (baby2 and face), we use the color amplification pipeline by setting `\"color_amp\": True`.  \n- The dictionary holds the file paths and processing parameters for each video.  \n- The loop loads each video, applies either the standard Eulerian motion magnification or the color-only amplification, saves the result, and cleans up memory.  \n- An optional grid search is enabled for the face video to further optimize parameters based on a Region Of Interest (ROI).\n\nAfter processing, you can analyze the saved AVI files (e.g., by watching them) and add further markdown cells to discuss your observations.\n","metadata":{}},{"cell_type":"code","source":"# Cell 6: Testing the Model on Videos\n\n# Define base path (adjust based on your dataset location)\nbase_path = \"/kaggle/input/final-cv-dataset\"\n\n# Create a dictionary for each video along with parameters.\n# For videos where we want only color change detection, set \"color_amp\": True.\nvideos = {\n    \"baby\": {\n         \"path\": os.path.join(base_path, \"baby.mp4\"),\n         \"params\": {\"freq_min\": 0.8, \"freq_max\": 2.0, \"amplification\": 50, \"pyramid_levels\": 4, \"skip_levels_at_top\": 1},\n         \"color_amp\": False\n    },\n    \"baby2\": {\n         \"path\": os.path.join(base_path, \"baby2.mp4\"),\n         \"params\": {\"freq_min\": 2.0, \"freq_max\": 2.5, \"amplification\": 100, \"pyramid_levels\": 4},\n         \"color_amp\": True\n    },\n    \"face\": {\n         \"path\": os.path.join(base_path, \"face.mp4\"),\n         \"params\": {\"freq_min\": 0.8, \"freq_max\": 1.0, \"amplification\": 80, \"pyramid_levels\": 5},\n         \"use_grid_search\": True,\n         \"roi\": (50,150,40,120),\n         \"color_amp\": True\n    },\n    \"wrist\": {\n         \"path\": os.path.join(base_path, \"wrist.mp4\"),\n         \"params\": {\"freq_min\": 0.4, \"freq_max\": 3.0, \"amplification\": 15, \"pyramid_levels\": 4, \"skip_levels_at_top\": 1},\n         \"color_amp\": False\n    }\n}\n\n# Loop through each video, process, and save output\nfor vid_name, vid_info in videos.items():\n    print(f\"\\nProcessing {vid_name} ...\")\n    video_path = vid_info[\"path\"]\n    if not os.path.isfile(video_path):\n        print(f\"File not found: {video_path}\")\n        continue\n    try:\n        vid_data, fps = load_video_no_downsample(video_path)\n        print(f\"{vid_name} loaded: shape={vid_data.shape}, fps={fps}\")\n    except Exception as e:\n        print(f\"Error loading {vid_name}: {e}\")\n        continue\n\n    params = vid_info[\"params\"]\n    if vid_info.get(\"use_grid_search\", False):\n        # Optionally, perform grid search to optimize parameters (for face video)\n        from IPython.display import clear_output\n        print(\"Running grid search for parameters ...\")\n        best_params, best_score = grid_search_eulerian_params(vid_data, fps, roi=vid_info.get(\"roi\", (0,50,0,50)))\n        if best_params:\n            print(f\"Grid search found best params = {best_params}, score={best_score}\")\n            params[\"freq_min\"], params[\"freq_max\"], params[\"amplification\"] = best_params\n        clear_output(wait=True)\n    if vid_info.get(\"color_amp\", False):\n        # Use color amplification pipeline\n        magnified = eulerian_color_amplification(vid_data, fps,\n                                                  freq_min=params[\"freq_min\"],\n                                                  freq_max=params[\"freq_max\"],\n                                                  amplification=params[\"amplification\"],\n                                                  pyramid_levels=params[\"pyramid_levels\"])\n    else:\n        # Use standard motion magnification\n        magnified = eulerian_magnification(vid_data, fps,\n                                            freq_min=params[\"freq_min\"],\n                                            freq_max=params[\"freq_max\"],\n                                            amplification=params[\"amplification\"],\n                                            pyramid_levels=params[\"pyramid_levels\"],\n                                            skip_levels_at_top=params.get(\"skip_levels_at_top\", 1))\n    \n    out_filename = f\"magnified_{vid_name}.avi\"\n    save_video_float32_as_avi(magnified, fps, out_filename)\n    del vid_data, magnified\n    gc.collect()\n\nprint(\"\\nAll processing done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:13:03.550249Z","iopub.execute_input":"2025-02-25T22:13:03.550521Z","iopub.status.idle":"2025-02-25T22:16:18.350658Z","shell.execute_reply.started":"2025-02-25T22:13:03.550494Z","shell.execute_reply":"2025-02-25T22:16:18.349785Z"}},"outputs":[{"name":"stdout","text":"Saved magnified_face.avi\n\nProcessing wrist ...\nwrist loaded: shape=(894, 352, 640, 3), fps=30.0\nSaved magnified_wrist.avi\n\nAll processing done!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Cell 7: Download Links for Processed Videos\n\n# List each processed AVI file (adjust file names as needed)\noutput_files = [\"magnified_baby.avi\", \"magnified_baby2.avi\", \"magnified_face.avi\", \"magnified_wrist.avi\"]\n\n# Display individual download links for each file\nfor file in output_files:\n    display(FileLink(file))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:18.352653Z","iopub.execute_input":"2025-02-25T22:16:18.352872Z","iopub.status.idle":"2025-02-25T22:16:18.362036Z","shell.execute_reply.started":"2025-02-25T22:16:18.352853Z","shell.execute_reply":"2025-02-25T22:16:18.361332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_baby.avi","text/html":"<a href='magnified_baby.avi' target='_blank'>magnified_baby.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_baby2.avi","text/html":"<a href='magnified_baby2.avi' target='_blank'>magnified_baby2.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_face.avi","text/html":"<a href='magnified_face.avi' target='_blank'>magnified_face.avi</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_wrist.avi","text/html":"<a href='magnified_wrist.avi' target='_blank'>magnified_wrist.avi</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Results Analysis & Conclusion\n\n","metadata":{}},{"cell_type":"markdown","source":"# 1)","metadata":{}},{"cell_type":"markdown","source":"## Analysis of \"baby.mp4\" and Its Processed Version\n\n### Objective\nThe aim was to enhance subtle motions related to physiological signals like breathing or heartbeat in \"baby.mp4\" using Eulerian Magnification.\n\n### Methodology\n- **Original Video (\"baby.mp4\")**: Examined for baseline movements and physiological indicators.\n- **Processed Video (\"magnified_baby.avi\")**: Reviewed for enhancements in motion detection.\n\n### Settings\n- **Frequency Range**: 0.8 Hz to 2.0 Hz, aimed at capturing typical breathing rates of babies.\n- **Amplification Factor**: Set at 50 to enhance motions without excessive distortion.\n- **Pyramid Levels**: Utilized 4 levels for a detailed multi-scale representation.\n- **Skip Levels at Top**: Excluded the top level to minimize noise amplification.\n\n### Observations\n\n#### 1. **Effectiveness of Motion Amplification**\n- **Original Video**: Shows minimal visible movement, primarily subtle breathing.\n- **Processed Video**: Enhanced breathing patterns are noticeable, indicating effective amplification.\n\n#### 2. **Visual Quality and Artifacts**\n- **Clarity**: Both videos maintain high image clarity.\n- **Artifacts**: No significant artifacts are observed, suggesting well-balanced settings.\n\n#### 3. **Temporal Consistency**\n- **Smoothness**: Motion appears smooth and consistent, with no abrupt changes, indicating accurate filtering.\n\n#### 4. **Realism and Usability**\n- **Natural Appearance**: Movements remain natural-looking, important for monitoring applications.\n- **Utility**: Enhanced visualization of subtle movements could aid in non-intrusive health monitoring.\n\n### Conclusion\nThe \"magnified_baby.avi\" effectively demonstrates Eulerian Magnification's application to enhance subtle physiological movements in infants. The technique's settings provided a good balance between amplification and natural appearance, making it valuable for monitoring infant well-being.\n\n### Next Steps\n- **Further Validation**: Test under various conditions and with infants in different states to validate robustness.\n- **Monitoring System Integration**: Consider integration with baby monitoring systems for real-time health alerts.\n- **Research Expansion**: Adapt techniques to monitor other subtle signals in different demographic groups.\n","metadata":{}},{"cell_type":"markdown","source":"# 2)","metadata":{}},{"cell_type":"markdown","source":"## Analysis of \"wrist.mp4\" and Its Processed Version\n\n### Objective\nThe goal was to enhance subtle movements related to pulse detection in the \"wrist.mp4\" video through Eulerian Magnification. This analysis aims to assess how effectively the video processing highlights these subtle physiological signals.\n\n### Methodology\n- **Original Video (\"wrist.mp4\")**: Reviewed to note baseline wrist movements or subtle pulse indications.\n- **Processed Video (\"magnified_wrist.avi\")**: Analyzed to determine the enhancement of wrist pulse movements.\n\n### Settings\n- **Frequency Range**: 0.4 Hz to 3.0 Hz, targeting the frequency range typical for human pulse rates.\n- **Amplification Factor**: Set at 15 to ensure subtle enhancement without introducing significant noise or distortion.\n- **Pyramid Levels**: 4 levels were used to create a detailed representation of the wrist movements.\n- **Skip Levels at Top**: The top level was skipped to minimize the amplification of high-frequency noise.\n\n### Observations\n\n#### 1. **Effectiveness of Motion Amplification**\n- **Original Video**: Shows minimal wrist movement with barely noticeable pulse motion.\n- **Processed Video**: There is a visible enhancement in the pulsating movement of the wrist, making the pulse more discernible.\n\n#### 2. **Visual Quality and Artifacts**\n- **Clarity**: Both videos maintain a high level of clarity, with no degradation due to processing.\n- **Artifacts**: Minimal artifacts are present, which indicates that the settings used are appropriate for this type of physiological signal enhancement.\n\n#### 3. **Temporal Consistency**\n- **Smoothness**: The enhanced motions in the processed video are consistent and smooth, indicating effective isolation of the desired frequency band.\n\n#### 4. **Realism and Usability**\n- **Natural Appearance**: The movements in the processed video remain realistic, which is crucial for applications where accurate pulse monitoring is necessary.\n- **Utility**: This enhanced visualization of wrist pulses can be particularly useful in medical diagnostics and remote health monitoring systems.\n\n### Conclusion\nThe processed \"magnified_wrist.avi\" demonstrates a successful application of Eulerian Magnification to visibly enhance the wrist's pulsating movements. This processing makes it easier to observe and analyze physiological signals that are otherwise too subtle to detect with the naked eye.\n\n### Next Steps\n- **Further Validation**: Additional testing with different lighting conditions and varying skin tones to validate the robustness of the technique.\n- **Integration into Health Monitoring Systems**: Explore integration with health monitoring systems for continuous pulse monitoring.\n- **Expansion to Other Physiological Signals**: Consider adapting this technique to enhance other subtle physiological signals for broader medical applications.\n","metadata":{}},{"cell_type":"markdown","source":"# 3)","metadata":{}},{"cell_type":"markdown","source":"## Analysis of \"face.mp4\" and Its Processed Version\n\n### Objective\nTo utilize Eulerian Video Magnification to enhance subtle color variations in the \"face.mp4\" video, focusing on areas that could indicate physiological changes such as heart rate or blood flow.\n\n### Methodology\n- **Original Video (\"face.mp4\")**: Analyzed to note baseline facial colorations and micro-movements.\n- **Processed Video (\"magnified_face.avi\")**: Evaluated for the efficacy in highlighting subtle color changes linked to cardiovascular activity.\n\n### Settings\n- **Frequency Range**: 0.8 Hz to 1.0 Hz, specifically chosen to capture the typical frequency of an adult human's resting heart rate.\n- **Amplification Factor**: Set to 80 to intensify subtle changes without distorting overall facial features.\n- **Pyramid Levels**: Utilized 5 levels to ensure detailed analysis and reconstruction.\n- **Region of Interest (ROI)**: Focused on (50,150,40,120), targeting specific facial areas likely to show color changes due to blood flow.\n- **Color Amplification**: True, indicating the enhancement was aimed solely at color changes, not motion.\n\n### Observations\n\n#### 1. **Effectiveness of Color Amplification**\n- **Original Video**: Displays normal facial tones with minimal discernible color fluctuation.\n- **Processed Video**: Shows enhanced visibility of color fluctuations that could correlate with pulse rate and oxygenation levels.\n\n#### 2. **Visual Quality and Artifacts**\n- **Clarity**: The enhancement preserves the clarity and integrity of facial features.\n- **Artifacts**: There are minimal artifacts, suggesting the settings are well-tuned to balance amplification with natural appearance.\n\n#### 3. **Temporal Consistency**\n- **Smoothness**: The amplification maintains temporal smoothness, indicating effective isolation and enhancement of the desired frequency band.\n\n#### 4. **Realism and Usability**\n- **Natural Appearance**: Despite the amplification, facial expressions and features remain realistic and undistorted.\n- **Utility**: This technique shows potential for non-invasive monitoring of physiological signs, which could be useful in medical settings or user health monitoring systems.\n\n### Conclusion\nThe \"magnified_face.avi\" effectively demonstrates the potential of Eulerian Video Magnification to enhance subtle facial color changes that are indicative of underlying physiological conditions. The technique could be particularly useful in scenarios requiring non-contact monitoring of an individual's health status.\n\n### Next Steps\n- **Clinical Testing**: Validate the technique's effectiveness and reliability in clinical trials or controlled settings.\n- **Algorithm Optimization**: Continue refining the parameters with additional grid search iterations to optimize for different skin tones and conditions.\n- **Expand Application Scope**: Explore the potential for using this technology in telemedicine, remote patient monitoring, and stress analysis.\n\n### Recommendations\n- **Further Research**: Investigate the correlation between visible facial color changes and specific health conditions.\n- **Integration with Diagnostic Tools**: Consider integration with diagnostic systems that utilize facial analysis for early detection of health issues.\n","metadata":{}},{"cell_type":"markdown","source":"# 4)","metadata":{}},{"cell_type":"markdown","source":"## Analysis of \"baby2.mp4\" and Its Processed Output\n\n### Objective\nThe goal for processing \"baby2.mp4\" was to reveal subtle color changes that correspond to physiological signals (such as pulse) in a newborn. In this implementation, we use a color-only amplification approach with the following parameters:\n- **Frequency Range:** 2.0–2.5 Hz (to capture the higher pulse rate typical in newborns)\n- **Amplification Factor:** 100 (to boost subtle signals)\n- **Pyramid Levels:** 4 (to balance detail preservation with computational efficiency)\n\n### Methodology Overview\n1. **Preprocessing:**  \n   The video is loaded in its original resolution and converted to a float32 representation (values normalized between 0 and 1).  \n2. **Color-Only Amplification:**  \n   - Each frame is downsampled through a Laplacian pyramid (4 levels) to obtain a coarse representation.\n   - A temporal bandpass filter is applied to the downsampled video to extract signal components within the 2.0–2.5 Hz frequency range.\n   - The filtered signal is then upsampled back to the original resolution and added to the original frames.\n3. **Output:**  \n   The processed video, saved as \"magnified_baby2.avi\", should display enhanced subtle color variations corresponding to the newborn's pulse.\n\n### Observations\n\n- **Enhanced Signal Visibility:**  \n  The processed video clearly reveals rhythmic color variations in the facial region, which are barely perceptible in the original video. This suggests that the selected frequency band and high amplification factor effectively highlight the physiological signal.\n\n- **Visual Quality:**  \n  Despite the high amplification factor (100), the video maintains good clarity with minimal artifacts. The use of a 4-level pyramid helps ensure that the amplification is applied uniformly, preserving overall image quality.\n\n- **Temporal Consistency:**  \n  The color changes appear smooth and consistent over time. This temporal consistency indicates that the temporal bandpass filter has successfully isolated the desired frequency components.\n\n- **Applicability:**  \n  The visible color changes in the processed video may be useful for non-invasive monitoring of newborn physiological signals. The result demonstrates that even subtle changes can be amplified effectively using Eulerian Video Magnification.\n\n### Conclusion\nThe Eulerian Color Amplification applied to \"baby2.mp4\" successfully enhances the subtle color changes associated with the newborn's physiological signals. With the chosen parameters (2.0–2.5 Hz, amplification of 100, and 4 pyramid levels), the output provides a clear, temporally consistent visualization of these signals while preserving overall video quality. This method shows promise for applications in non-invasive health monitoring.\n\n### Next Steps\n- **Validation:** Compare the detected signals with clinical measurements to validate the method.\n- **Parameter Tuning:** Further adjustments to the frequency range and amplification factor might optimize the balance between signal visibility and noise.\n- **Broader Testing:** Apply the technique to additional newborn videos to assess robustness and generalizability.\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2)  my own dataset","metadata":{}},{"cell_type":"markdown","source":"\n# Testing Eulerian Color Amplification on My Own Video\n\n\n\n \n### *Subtle Color Changes Detection with Optimized Parameters*\n\n---\n\n### **Objective**  \nThis implementation applies **Eulerian Color Amplification (ECA)** on the *\"face3.mp4\"* video to detect **subtle facial color changes**. The parameters are optimized to balance **high-quality output** and **realistic amplification**, reducing noise and avoiding over-exaggeration.\n\n---\n\n### **Methodology Overview**  \nThe Eulerian Color Amplification process includes the following key steps:\n\n1. **Preprocessing**:  \n   - **Downsampling** to 640×360 resolution for memory efficiency.  \n   - **Denoising** using Gaussian blur to suppress high-frequency noise.  \n   - **Frame Limiting** to 300 frames for computational optimization.  \n\n2. **Temporal Bandpass Filtering**:  \n   - **Frequency Range**: 0.7–1.2 Hz, targeting physiological signals such as heart rate.  \n   - **Amplification**: Factor of 30 to achieve visible but subtle enhancements.  \n\n3. **Color-Only Amplification**:  \n   - Implemented via a **5-level pyramid decomposition** to capture fine details.  \n   - **Cubic interpolation** during upsampling for smooth, high-quality video reconstruction.  \n\n4. **Reconstruction and Saving**:  \n   - The amplified color signal is combined with the original frames.  \n   - The output video is saved in AVI format using the **XVID codec** for efficient playback.\n\n---\n\n### **Key Parameters and Rationale**  \n- **Frequency Range (0.7–1.2 Hz)**: Corresponds to typical adult heart rate frequencies (42–72 bpm).  \n- **Amplification Factor (30)**: Ensures subtle enhancement without introducing distortion.  \n- **Pyramid Levels (5)**: Offers a balance between processing time and detail preservation.  \n- **Gaussian Denoising**: Prevents the amplification of irrelevant noise.\n\n---\n\n### **Output Details**  \n- **Output File**: `magnified_face3_lower_amp.avi`  \n- **Video Codec**: XVID  \n- **Resolution**: 640×360  \n- **Result**: The final video highlights **subtle facial color changes** with **natural appearance** and **minimal noise**, making it suitable for applications such as non-invasive physiological monitoring and biometric analysis.\n\n---\n\n### **Conclusion**  \nThis optimized implementation of **Eulerian Color Amplification** achieves **high-quality visualization** of **subtle facial color variations**. The chosen parameters result in a natural and stable output, demonstrating the effectiveness of ECA for detecting physiological signals with minimal noise and distortion.\n\n","metadata":{}},{"cell_type":"code","source":"# =======================================================\n# FINAL CELL: Process \"face3.mp4\" for Subtle Color Changes (Optimized for Lower Amplification)\n# =======================================================\n\nimport cv2, numpy as np, scipy.fftpack, os, gc\nfrom IPython.display import FileLink, display\n\n# ----- Step 1: Load Video with Downsampling, Denoising & Frame Limiting -----\ndef load_video_downsampled_denoise(video_path, max_frames=300, width=640, height=360):\n    if not os.path.isfile(video_path):\n        raise FileNotFoundError(f\"Video not found: {video_path}\")\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = []\n    count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret or count >= max_frames:\n            break\n        frame = cv2.resize(frame, (width, height))\n        # Apply mild denoising with a Gaussian blur\n        frame = cv2.GaussianBlur(frame, (3,3), 0)\n        frames.append(frame.astype(np.float32) / 255.0)\n        count += 1\n    cap.release()\n    return np.array(frames, dtype=np.float32), fps\n\n# ----- Step 2: Temporal Bandpass Filter -----\ndef temporal_bandpass_filter(data, fps, freq_min, freq_max, amplification=1.0):\n    fft_data = scipy.fftpack.rfft(data, axis=0)\n    freqs = scipy.fftpack.rfftfreq(data.shape[0], d=1.0/fps)\n    low_idx = np.argmin(np.abs(freqs - freq_min))\n    high_idx = np.argmin(np.abs(freqs - freq_max))\n    fft_data[:low_idx] = 0\n    fft_data[high_idx+1:] = 0\n    return scipy.fftpack.irfft(fft_data, axis=0) * amplification\n\n# ----- Step 3: Eulerian Color Amplification (Optimized for Lower Amplification) -----\ndef eulerian_color_amplification_improved(vid, fps, freq_min, freq_max, amplification, pyramid_levels=5):\n    nframes, orig_h, orig_w, _ = vid.shape\n    coarse_frames = []\n    for i in range(nframes):\n        frame = vid[i]\n        for _ in range(pyramid_levels - 1):\n            frame = cv2.pyrDown(frame)\n        coarse_frames.append(frame)\n    coarse_video = np.array(coarse_frames, dtype=np.float32)\n    \n    filtered = coarse_video + temporal_bandpass_filter(coarse_video, fps, freq_min, freq_max, amplification)\n    \n    up_frames = []\n    for i in range(nframes):\n        up_frame = cv2.resize(filtered[i], (orig_w, orig_h), interpolation=cv2.INTER_CUBIC)\n        combined = np.clip(vid[i] + up_frame, 0, 1)\n        up_frames.append(combined)\n    return np.array(up_frames, dtype=np.float32)\n\n# ----- Step 4: Save Video Function -----\ndef save_video(video, fps, filename=\"output.avi\"):\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    h, w = video.shape[1:3]\n    out = cv2.VideoWriter(filename, fourcc, fps, (w, h), True)\n    for frame in video:\n        out.write(np.clip(frame * 255.0, 0, 255).astype(np.uint8))\n    out.release()\n    print(f\"Saved {filename}\")\n\n# ----- Step 5: Process \"face3.mp4\" with Lower Amplification -----\nvideo_path = \"/kaggle/input/final-cv-dataset/face3.mp4\"\nvid_data, fps = load_video_downsampled_denoise(video_path, max_frames=300, width=640, height=360)\nprint(f\"Loaded face3.mp4: shape={vid_data.shape}, fps={fps:.2f}\")\n\n# Set parameters: use a slightly broader band if desired, and lower amplification.\nfreq_min, freq_max = 0.7, 1.2  # it can adjust as needed\namplification = 30             # reduced amplification for a more subtle effect\npyramid_levels = 5\n\nresult = eulerian_color_amplification_improved(vid_data, fps, freq_min, freq_max, amplification, pyramid_levels)\nsave_video(result, fps, \"magnified_face3_lower_amp.avi\")\ndel vid_data, result\ngc.collect()\n\n# ----- Step 6: Download Link -----\ndisplay(FileLink(\"magnified_face3_lower_amp.avi\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:18.363326Z","iopub.execute_input":"2025-02-25T22:16:18.363565Z","iopub.status.idle":"2025-02-25T22:16:23.327583Z","shell.execute_reply.started":"2025-02-25T22:16:18.363546Z","shell.execute_reply":"2025-02-25T22:16:23.326750Z"}},"outputs":[{"name":"stdout","text":"Loaded face3.mp4: shape=(300, 360, 640, 3), fps=30.07\nSaved magnified_face3_lower_amp.avi\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/magnified_face3_lower_amp.avi","text/html":"<a href='magnified_face3_lower_amp.avi' target='_blank'>magnified_face3_lower_amp.avi</a><br>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Comparative Analysis of Processed and Original Videos\n\n### Introduction\nThis analysis examines the effectiveness of the Eulerian Color Amplification technique applied on \"face3.mp4\" resulting in \"magnified_face3_lower_amp (1).avi\". The aim was to subtly amplify color changes due to physiological signals without introducing noise or unrealistic artifacts.\n\n### Methodology\n- **Original Video (\"face3.mp4\")**: Reviewed for baseline color variations and video quality.\n- **Processed Video (\"magnified_face3_lower_amp (1).avi\")**: Examined for enhanced visualization of subtle physiological changes.\n\n### Observations and Findings\n\n#### 1. Visual Quality\n- **Original Video**: Displays consistent lighting and color tones with natural human skin colors.\n- **Processed Video**: Maintains resolution and general clarity, no introduction of pixelation or significant noise, suggesting effective noise control measures.\n\n#### 2. Color Change Detection\n- **Subtlety and Naturalness**: The processed video reveals enhanced subtle color changes on the facial region, possibly indicative of blood flow or heart rate variations. These enhancements are subtle enough not to distort the overall appearance.\n- **Comparison with Original**: Compared to the original, these changes are not noticeable without the amplification, validating the effectiveness of the applied technique in revealing these subtle physiological signals.\n\n#### 3. Noise and Artifacts\n- **Noise Level**: The processed video shows minimal noise, which is an improvement considering the amplification applied. The Gaussian blur preprocessing step likely helped in achieving this clarity.\n- **Artifacts**: There are no significant compression or processing artifacts, indicating a well-tuned amplification process.\n\n#### 4. Temporal Consistency and Smoothness\n- **Flow of Changes**: The color changes in the processed video appear smooth and consistent over time, without abrupt or unnatural transitions, suggesting that the temporal bandpass filter was appropriately set to capture the relevant physiological frequencies.\n- **Realism and Practicality**: The rhythmic nature of the color changes correlates well with expected physiological behaviors, enhancing the video's utility for observational studies or diagnostic purposes.\n\n### Conclusion\nThe processed video \"magnified_face3_lower_amp (1).avi\" successfully enhances subtle physiological color variations with a high degree of realism and without degrading video quality. This indicates that the chosen parameters for the Eulerian Color Amplification—specifically the frequency range, amplification factor, and pyramid levels—were well-selected to maximize visibility of subtle changes while maintaining the natural appearance of the video.\n\nThe technique proves to be a valuable tool for enhancing subtle physiological indicators in video data, which could be particularly useful in medical diagnostic processes where non-invasive monitoring is desired.\n\n### Recommendations\n- **Further Studies**: Additional testing with different physiological conditions and in varied lighting settings could help in understanding the robustness of the technique.\n- **Application Development**: Incorporating this technique into real-time processing applications for health monitoring could be explored.\n","metadata":{}},{"cell_type":"markdown","source":"# 🚨 **Attention, Please!** 🚨  \n### **Eulerian Video Magnification Implementation**\n","metadata":{}},{"cell_type":"markdown","source":"# Extended Eulerian Video Magnification: Comparing Initial vs. Final Implementations\n\nThis document **compares** the earlier, more **basic** implementation of Eulerian Video Magnification (EVM) with the **expanded** version that incorporates additional methods inspired by the original research paper:\n  \n> **H. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman**.  \n> *Eulerian Video Magnification for Revealing Subtle Changes in the World.*  \n> ACM Transactions on Graphics (Proc. SIGGRAPH), 2012.\n\nBy tracing the **first steps** through the **final step**, we see how each stage was refined to **match** the paper’s recommended approach, yielding **more accurate** and **visually appealing** magnification results.\n\n---\n\n## 1. Early vs. Expanded Approach: An Overview\n\n### Early Implementation\n1. **Simple Video I/O**: Reading video frames, normalizing them, then writing out the result.  \n2. **Basic Color Space**: Typically operated in **RGB** without conversions, risking color distortions when large amplification was applied.  \n3. **No Face Detection**: Magnification was applied to **entire frames**, which could amplify background noise or irrelevant movements.  \n4. **Laplacian Pyramid**: Used but sometimes with **fixed levels** and no adaptive strategy to prevent over-amplification.  \n5. **Temporal Filtering**: Often a basic bandpass filter or naive frequency selection, leading to less precise isolation of the target signal.\n\n### Expanded (Paper-Based) Implementation\n1. **Color Space Conversions (RGB <-> YIQ)**: Following Wu et al.’s recommendation, we separate **luminance (Y)** from **chrominance (I, Q)** to preserve color fidelity when amplifying subtle signals.  \n2. **Face Mask Generation**: Leveraging **Haar Cascade** detection, we apply magnification **only** within the facial region to focus on relevant signals (e.g., pulse) and reduce background noise.  \n3. **Multiscale Laplacian Pyramid**: Adopting a **coarse-to-fine** decomposition so that each spatial frequency band can be filtered and amplified **independently**. This matches the paper’s “spatial decomposition” strategy.  \n4. **Temporal Butterworth Bandpass Filter**: Implementing a **zero-phase** bandpass filter (via `scipy.signal.butter` and `filtfilt`) to isolate a precise frequency range (e.g., ~0.8–1 Hz for heartbeat).  \n5. **Adaptive Amplification**: Following the paper’s derivation (Eq. 14 in Wu et al.), we **scale** the amplification factor \\(\\alpha\\) based on the pyramid level to avoid artifacts at higher spatial frequencies.  \n6. **Reconstruction**: Collapsing the pyramid and converting back to **RGB** carefully, ensuring minimal color clipping.\n\n---\n\n## 2. Detailed Step-by-Step Comparison\n\nBelow is a **side-by-side** look at how each step evolved from the **initial** to the **final** approach.\n\n### **Step 1: Imports & Basic Setup**\n- **Before**: Simple imports (`cv2`, `numpy`) with minimal environment checks.  \n- **Now**: Includes scientific libraries (`scipy.signal`) for Butterworth filters and robust error handling (`os`) to ensure valid video paths.\n\n### **Step 2: Video I/O Utilities**\n- **Before**: Directly read frames, no special normalization or error checks.  \n- **Now**:  \n  - Normalization to \\([0,1]\\) float range to **stabilize** computations.  \n  - Proper error handling if file not found.  \n  - Flexible output codecs (e.g., `'XVID'`) and shape checks to **match** the original resolution.\n\n### **Step 3: Color Space Conversions (RGB <-> YIQ)**\n- **Before**: Stayed in **RGB** only, risking color channel clipping when magnified.  \n- **Now**:  \n  - **Matrix transforms** for RGB \\(\\leftrightarrow\\) YIQ.  \n  - Paper emphasizes separating **luminance** from **chrominance** to better preserve color and **reduce artifacts**.\n\n### **Step 4: Face Mask Generation**\n- **Before**: None or a rudimentary ROI approach (possibly ignoring background).  \n- **Now**:  \n  - **Haar Cascade** to detect faces in each frame (or at least in the first frame).  \n  - Create a **binary mask** where the face is 1 and the rest is 0.  \n  - Magnify only within the face, reducing background flicker.\n\n### **Step 5: Laplacian Pyramid Construction**\n- **Before**: Possibly used a **Gaussian pyramid** or direct pixel domain.  \n- **Now**:  \n  - Laplacian pyramid with multiple **levels**.  \n  - Each level captures **different spatial frequencies**, following the paper’s approach to selectively amplify certain scales.\n\n### **Step 6: Temporal Butterworth Bandpass Filter**\n- **Before**: Might have used a **basic** bandpass or naive frequency cutoff.  \n- **Now**:  \n  - A **zero-phase** Butterworth filter (`filtfilt`) is applied to each level’s **temporal** signal.  \n  - Precisely isolates **physiological** frequency (e.g., 0.8–1.0 Hz for heartbeat).  \n  - Minimizes phase distortion, consistent with the **paper’s** emphasis on stable, artifact-free magnification.\n\n### **Step 7: Face Mask Resizing (Per Pyramid Level)**\n- **Before**: If a mask was used, it was static, ignoring **pyramid-level dimensions**.  \n- **Now**:  \n  - **Resize** the face mask for each pyramid level to match the smaller or bigger resolution at that level.  \n  - Ensures **consistent** region-of-interest across scales.\n\n### **Step 8: Multiscale Eulerian Video Magnification**\n- **Before**: A single function that combined everything **without** a clear separation of steps or adaptive scaling.  \n- **Now**:  \n  - Precisely matches Wu et al. (2012):  \n    1. **Spatial decomposition** (Laplacian pyramid).  \n    2. **Temporal bandpass** filtering each level.  \n    3. **Adaptive amplification**: \\(\\alpha_{\\text{level}} = \\frac{\\alpha}{2^{( \\text{pyramid\\_levels} - \\text{level} - 1 )}}\\).  \n    4. **Add filtered signal** back.  \n    5. **Reconstruct** final frames, ensuring minimal color clipping.\n\n---\n\n## 3. Methods Added from the Research Paper\n\n1. **First-Order Motion Approximation**  \n   - Paper shows that small motions can be **linearly approximated** and magnified without explicit optical flow.  \n2. **Frequency Range Selection**  \n   - The approach to **select** \\((\\text{freq\\_min}, \\text{freq\\_max})\\) is guided by typical physiological or mechanical signals (e.g., heartbeat ~1 Hz, breathing ~0.3 Hz).  \n3. **Masking & Spatial Pooling**  \n   - Wu et al. emphasize **reducing noise** by focusing on **lower spatial frequencies** and/or using **region-of-interest** masks.  \n4. **Multiscale Analysis**  \n   - Derived formula \\((1 + \\alpha)\\delta(t) < \\lambda / 8\\) to avoid **artifacts** at high frequencies.  \n   - Our final code **attenuates** \\(\\alpha\\) at higher pyramid levels accordingly.\n\n---\n\n## 4. Conclusion\n\n**Comparing the earlier, simpler pipeline** to the **expanded, paper-based** implementation reveals significant improvements in:\n- **Color fidelity** (via YIQ conversions),\n- **Region-of-interest** targeting (face masks),\n- **Noise reduction** (pyramid-level processing + Butterworth bandpass),\n- **Reduced artifacts** (adaptive \\(\\alpha\\) scaling),\n- **Better alignment** with the **original Eulerian Video Magnification paper** by Wu et al.\n\nThese refinements collectively **achieve the exact approach** recommended in the research paper, yielding **robust** and **high-quality** magnification of subtle changes (like pulse, small vibrations, or micro-expressions) while **minimizing** unintended amplification of noise or background.\n\n---\n\n**References**  \n- H.-Y. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman.  \n  *Eulerian Video Magnification for Revealing Subtle Changes in the World.*  \n  ACM Transactions on Graphics (Proc. SIGGRAPH), 2012.\n","metadata":{}},{"cell_type":"markdown","source":"\n\n## Step 1: Imports & Basic Setup\n\n**Purpose**  \n- Import the necessary Python libraries:\n  - `cv2` for computer vision tasks.\n  - `numpy` for numerical array operations.\n  - `scipy.signal` for the Butterworth filter.\n  - `os` for file handling.\n\n\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 1: IMPORTS & BASIC SETUP\n# ===============================================\nimport cv2\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.328258Z","iopub.execute_input":"2025-02-25T22:16:23.328470Z","iopub.status.idle":"2025-02-25T22:16:23.331948Z","shell.execute_reply.started":"2025-02-25T22:16:23.328452Z","shell.execute_reply":"2025-02-25T22:16:23.331112Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Formulas/Concepts**  \n- Loading a video involves reading each frame via OpenCV’s `VideoCapture`.\n- Saving a video uses OpenCV’s `VideoWriter` with a chosen codec (here, 'XVID').\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 2: VIDEO I/O UTILITIES\n# ===============================================\ndef load_video(path):\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"Video not found: {path}\")\n    cap = cv2.VideoCapture(path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        # Normalize to [0, 1]\n        frames.append(frame.astype(np.float32) / 255.0)\n    cap.release()\n    return np.array(frames), fps\n\ndef save_video(video, fps, filename):\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    h, w = video.shape[1:3]\n    out = cv2.VideoWriter(filename, fourcc, fps, (w, h), True)\n    for frame in video:\n        out.write((frame * 255).astype(np.uint8))\n    out.release()\n    print(f\"Saved {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.332846Z","iopub.execute_input":"2025-02-25T22:16:23.333268Z","iopub.status.idle":"2025-02-25T22:16:23.349033Z","shell.execute_reply.started":"2025-02-25T22:16:23.333239Z","shell.execute_reply":"2025-02-25T22:16:23.348222Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Step 3: Color Space Conversions (RGB &lt;--&gt; YIQ)</title>\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      line-height: 1.6;\n      margin: 20px;\n      max-width: 900px;\n    }\n    code {\n      background-color: #f4f4f4;\n      padding: 4px;\n      font-family: Consolas, \"Courier New\", monospace;\n    }\n    pre {\n      background-color: #f9f9f9;\n      padding: 10px;\n      overflow-x: auto;\n      border-left: 4px solid #ccc;\n    }\n    h1, h2, h3 {\n      margin-top: 1.2em;\n      margin-bottom: 0.6em;\n    }\n    hr {\n      margin: 2em 0;\n    }\n    .matrix {\n      font-size: 1.1em;\n      margin-left: 2em;\n      margin-top: 1em;\n      margin-bottom: 1em;\n      line-height: 1.4;\n    }\n    .matrix p {\n      margin: 0;\n    }\n  </style>\n</head>\n<body>\n\n<h1>Step 3: Color Space Conversions (RGB &lt;--&gt; YIQ)</h1>\n\n<p>\n  Converting between <strong>RGB</strong> and <strong>YIQ</strong> color spaces can help separate \n  <em>luminance (Y)</em> from <em>chrominance (I and Q)</em> components, which can sometimes yield \n  better magnification results without distorting color too much.\n</p>\n\n<hr>\n\n<h2>Formulas</h2>\n\n<h3>Converting from RGB to YIQ</h3>\n<div class=\"matrix\">\n  <p>\n    <strong>\n    &lpar; Y<br>\n           I<br>\n           Q &rpar;\n    </strong>\n    =\n    <strong>\n    &lpar; 0.299 &nbsp;&nbsp; 0.587 &nbsp;&nbsp; 0.114<br>\n            0.596 &nbsp;&nbsp;-0.274 &nbsp;&nbsp;-0.322<br>\n            0.211 &nbsp;&nbsp;-0.523 &nbsp;&nbsp; 0.312 &rpar;\n    </strong>\n    &times;\n    <strong>\n    &lpar; R<br>\n           G<br>\n           B &rpar;\n    </strong>\n  </p>\n</div>\n\n<p>\n  The matrix on the right transforms an <em>RGB</em> triplet into <em>YIQ</em>, \n  separating luminance (<strong>Y</strong>) from two chrominance components (<strong>I</strong> and <strong>Q</strong>).\n</p>\n\n<h3>Converting from YIQ back to RGB</h3>\n<p>\n  We use the <em>inverse</em> of the above matrix. In code, we often rely on \n  <code>np.linalg.inv</code> to compute it precisely.\n</p>\n\n<hr>\n\n\n\n<h2>Summary</h2>\n<p>\n  Switching between <strong>RGB</strong> and <strong>YIQ</strong> helps separate luminance from \n  color channels, making <em>Eulerian Video Magnification</em> more robust to \n  color distortion. By working in YIQ space, we can amplify subtle signals (e.g., \n  pulse or breathing changes) in the <em>luminance channel</em> without introducing \n  strong artifacts in the color channels.\n</p>\n\n</body>\n</html>\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 3: COLOR SPACE CONVERSIONS (RGB <-> YIQ)\n# ===============================================\ndef rgb_to_yiq(img):\n    transform = np.array([[0.299, 0.587, 0.114],\n                          [0.596, -0.274, -0.322],\n                          [0.211, -0.523, 0.312]])\n    return np.dot(img, transform.T)\n\ndef yiq_to_rgb(img):\n    transform = np.linalg.inv(np.array([[0.299, 0.587, 0.114],\n                                        [0.596, -0.274, -0.322],\n                                        [0.211, -0.523, 0.312]]))\n    return np.dot(img, transform.T)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.349885Z","iopub.execute_input":"2025-02-25T22:16:23.350101Z","iopub.status.idle":"2025-02-25T22:16:23.366384Z","shell.execute_reply.started":"2025-02-25T22:16:23.350084Z","shell.execute_reply":"2025-02-25T22:16:23.365724Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Step 4: Face Mask Generation (Haar Cascades)\n\n### Purpose\nUse OpenCV’s **Haar Cascade** classifier to detect the face region in the frame and generate a **binary mask**.  \nThis version is a simple demonstration that:  \n- Sets pixels **inside** the detected face rectangle to **1**.  \n- Sets pixels **outside** the rectangle to **0**.  \n\n---\n\n### Concept\n\n- **Haar Cascades** are a classic object detection method using features and a trained cascade of classifiers.  \n- The function `detectMultiScale` returns **bounding boxes** for faces in the grayscale image.\n\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 4: FACE MASK GENERATION (Haar Cascades)\n# ===============================================\ndef generate_face_mask(frame):\n    gray = (frame[:, :, 0] * 255).astype(np.uint8)\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n    mask = np.zeros_like(gray, dtype=np.float32)\n    for (x, y, w, h) in faces:\n        mask[y:y + h, x:x + w] = 1.0\n    return np.expand_dims(mask, axis=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.367101Z","iopub.execute_input":"2025-02-25T22:16:23.367294Z","iopub.status.idle":"2025-02-25T22:16:23.381840Z","shell.execute_reply.started":"2025-02-25T22:16:23.367261Z","shell.execute_reply":"2025-02-25T22:16:23.381228Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Step 5: Laplacian Pyramid Construction</title>\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      line-height: 1.6;\n      margin: 20px;\n      max-width: 900px;\n    }\n    code {\n      background-color: #f4f4f4;\n      padding: 4px;\n      font-family: Consolas, \"Courier New\", monospace;\n    }\n    pre {\n      background-color: #f9f9f9;\n      padding: 10px;\n      overflow-x: auto;\n      border-left: 4px solid #ccc;\n    }\n    h1, h2, h3 {\n      margin-top: 1.2em;\n      margin-bottom: 0.6em;\n    }\n    hr {\n      margin: 2em 0;\n    }\n    .formula {\n      font-size: 1.1em;\n      margin-left: 2em;\n    }\n  </style>\n</head>\n<body>\n\n<h1>Step 5: Laplacian Pyramid Construction</h1>\n\n<p>\n  A <strong>Laplacian pyramid</strong> is built for each frame to decompose the image into \n  multiple <em>spatial frequency bands</em>. This approach helps isolate subtle details \n  at different scales, which can then be selectively amplified in later steps \n  (e.g., via temporal filtering such as a Butterworth bandpass).\n</p>\n\n<hr>\n\n<h2>Purpose</h2>\n<p>\n  <strong>Why a Laplacian Pyramid?</strong>  \n  By splitting the image into coarse-to-fine representations, we can apply \n  <em>Eulerian Video Magnification</em> at specific spatial scales, reducing noise \n  and focusing on relevant signals.\n</p>\n\n<hr>\n\n<h2>Formulas</h2>\n\n<h3>Building the Laplacian Pyramid</h3>\n<p class=\"formula\">\n  L<sub>i</sub> = I<sub>i</sub> &minus; pyrUp(pyrDown(I<sub>i</sub>))\n</p>\n<p>\n  Where <strong>I<sub>i</sub></strong> is the image (or the Laplacian level from the previous iteration), \n  and <strong>L<sub>i</sub></strong> is the Laplacian at level <em>i</em>.  \n  <code>pyrDown</code> reduces the image size by half, and <code>pyrUp</code> upsamples back to \n  the original dimensions, allowing us to isolate the difference (Laplacian).\n</p>\n\n<h3>Collapsing the Laplacian Pyramid</h3>\n<p class=\"formula\">\n  I = &Sigma;<sub>i=0..n</sub> [ pyrUp(I<sub>i+1</sub>) + L<sub>i</sub> ]\n</p>\n<p>\n  Reconstructing the original image (or an amplified version of it) is done by \n  iteratively <code>pyrUp</code>-ing each level and adding the stored Laplacian details.\n</p>\n\n<hr>\n\n\n\n<h2>Key Points</h2>\n<ul>\n  <li>\n    <strong>Multiscale Representation:</strong> Each level isolates details of a particular\n    spatial frequency range.\n  </li>\n  <li>\n    <strong>Smooth Reconstruction:</strong> Using <code>pyrDown</code> and <code>pyrUp</code> \n    consistently ensures we can <em>collapse</em> the pyramid back to an approximate \n    original frame.\n  </li>\n  <li>\n    <strong>Efficiency:</strong> Operating at multiple levels allows us to selectively \n    amplify or filter specific scales (e.g., small facial color changes vs. larger \n    movements).\n  </li>\n</ul>\n\n</body>\n</html>\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 5: LAPLACIAN PYRAMID CONSTRUCTION\n# ===============================================\ndef build_laplacian_pyramid(frame, levels=5):\n    pyramid = []\n    current = frame\n    for _ in range(levels):\n        down = cv2.pyrDown(current)\n        up = cv2.pyrUp(down, dstsize=(current.shape[1], current.shape[0]))\n        lap = current - up\n        pyramid.append(lap)\n        current = down\n    pyramid.append(current)\n    return pyramid\n\ndef collapse_laplacian_pyramid(pyramid):\n    output = pyramid[-1]\n    for lvl in reversed(range(len(pyramid) - 1)):\n        up = cv2.pyrUp(output, dstsize=(pyramid[lvl].shape[1], pyramid[lvl].shape[0]))\n        output = pyramid[lvl] + up\n    return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.382651Z","iopub.execute_input":"2025-02-25T22:16:23.382899Z","iopub.status.idle":"2025-02-25T22:16:23.396796Z","shell.execute_reply.started":"2025-02-25T22:16:23.382868Z","shell.execute_reply":"2025-02-25T22:16:23.395921Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"<!DOCTYPE html>\n<html>\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Step 6: Temporal Butterworth Bandpass Filter</title>\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      line-height: 1.6;\n      margin: 20px;\n      max-width: 800px;\n    }\n    code {\n      background-color: #f4f4f4;\n      padding: 4px;\n      font-family: Consolas, \"Courier New\", monospace;\n    }\n    pre {\n      background-color: #f9f9f9;\n      padding: 10px;\n      overflow-x: auto;\n      border-left: 4px solid #ccc;\n    }\n    h1, h2, h3 {\n      margin-top: 1.2em;\n      margin-bottom: 0.6em;\n    }\n    hr {\n      margin: 2em 0;\n    }\n  </style>\n</head>\n<body>\n\n<h1>Step 6: Temporal Butterworth Bandpass Filter</h1>\n\n<p>\n  We apply a <strong>bandpass filter</strong> in the <em>temporal dimension</em> of each pixel in the video.\n  By focusing on a specific frequency range (for example, 0.8–1.0 Hz), we can isolate subtle physiological\n  signals (such as heartbeat) while minimizing irrelevant motions or noise outside this band.\n</p>\n\n<hr>\n\n<h2>Formulas</h2>\n\n<h3>General Butterworth Bandpass Filter (Continuous Form)</h3>\n\n<p>\n  A Butterworth bandpass filter can be represented as the product of a\n  <em>low-pass</em> and a <em>high-pass</em> Butterworth filter. In continuous form:\n</p>\n\n<p style=\"font-size: 1.1em; margin-left: 2em;\">\n  <strong>H(&omega;)</strong> = \n  &bigl;( &omega;<sub>H</sub><sup>n</sup> / (&omega;<sup>n</sup> + &omega;<sub>H</sub><sup>n</sup>) )&bigr; \n  &times;\n  &bigl;( &omega;<sup>n</sup> / (&omega;<sup>n</sup> + &omega;<sub>L</sub><sup>n</sup>) )&bigr;\n</p>\n\n<ul>\n  <li><strong>&omega;<sub>L</sub></strong> and <strong>&omega;<sub>H</sub></strong> are the low and high cutoff frequencies.</li>\n  <li><strong>n</strong> is the filter order (e.g., 2 or 3).</li>\n  <li><strong>&omega;</strong> is the angular frequency.</li>\n</ul>\n\n<p>\n  In a discrete implementation, these cutoff frequencies are converted to\n  normalized values relative to the <em>Nyquist frequency</em>\n  (sampling_rate / 2).\n</p>\n\n<h3>Discrete Implementation (Python <code>scipy.signal</code>)</h3>\n<p>\n  In Python, we typically use <code>scipy.signal.butter</code> to design the filter \n  and <code>scipy.signal.filtfilt</code> to apply zero-phase filtering, which reduces \n  phase distortion by filtering forward and backward.\n</p>\n\n<hr>\n\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 6: TEMPORAL BUTTERWORTH BANDPASS FILTER\n# ===============================================\ndef butter_bandpass_filter(data, fps, freq_min, freq_max, order=3):\n    nyquist = 0.5 * fps\n    low, high = freq_min / nyquist, freq_max / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    original_shape = data.shape\n    reshaped = data.reshape((original_shape[0], -1))\n    filtered = filtfilt(b, a, reshaped, axis=0)\n    return filtered.reshape(original_shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.397528Z","iopub.execute_input":"2025-02-25T22:16:23.397777Z","iopub.status.idle":"2025-02-25T22:16:23.415417Z","shell.execute_reply.started":"2025-02-25T22:16:23.397751Z","shell.execute_reply":"2025-02-25T22:16:23.414621Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Step 7: Face Mask Generation (Resized per Level)\n\n## Purpose\nIn this step, we **refine the face mask generation** to ensure that the facial region is accurately detected and **properly resized** for each level of the Laplacian pyramid. This approach focuses magnification only on the face and avoids amplifying irrelevant background areas.\n\n---\n\n## Concept\n1. **Grayscale Conversion**: We convert each video frame to grayscale using OpenCV’s `cvtColor`.  \n2. **Haar Cascade Detection**: The grayscale frame is passed to a Haar Cascade classifier (e.g., `haarcascade_frontalface_default.xml`), which returns bounding boxes for detected faces.  \n3. **Binary Mask Creation**: A binary mask (with values of 1 for face pixels and 0 for non-face pixels) is generated for the detected bounding box region.  \n4. **Resizing**: To ensure the mask aligns with each Laplacian pyramid level, the mask is **resized** accordingly at each level before applying magnification.\n\nBy restricting magnification to the **facial region**, we reduce noise and artifacts in the rest of the frame, leading to cleaner magnification results.\n\n---","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 7: FACE MASK GENERATION (Resized per level)\n# ===============================================\ndef generate_face_mask(frame):\n    \"\"\"\n    Detects the face in the frame using Haar cascades and returns a binary mask.\n    \"\"\"\n    gray = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n    \n    mask = np.zeros_like(gray, dtype=np.float32)\n    for (x, y, w, h) in faces:\n        mask[y:y + h, x:x + w] = 1.0\n    return mask[..., np.newaxis]  # Add channel dimension\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.416061Z","iopub.execute_input":"2025-02-25T22:16:23.416272Z","iopub.status.idle":"2025-02-25T22:16:23.429913Z","shell.execute_reply.started":"2025-02-25T22:16:23.416254Z","shell.execute_reply":"2025-02-25T22:16:23.429297Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Step 8: Multiscale Eulerian Video Magnification (With Resizing)\n\n## Purpose\nThis step implements the **overall pipeline** for Eulerian Video Magnification (EVM). The goal is to highlight subtle changes in the video, such as physiological signals (e.g., heartbeat), by amplifying specific frequency bands within the video frames.\n\n### **Process Overview:**\n1. **Load the video** and extract frames.\n2. **Convert frames to YIQ color space** to separate luminance and chrominance components, which improves color fidelity during magnification.\n3. **Detect the face region** in the first frame and generate a binary mask to restrict magnification to the face.\n4. **Build the Laplacian pyramid** for each frame to decompose it into multiple spatial frequency bands.\n5. **Apply the face mask** to each pyramid level for every frame.\n6. **Filter the temporal sequence** at each level using a **Butterworth bandpass filter** to isolate the target frequency band.\n7. **Amplify the filtered signals** by a specified amplification factor.\n8. **Collapse the Laplacian pyramid** to reconstruct the frames.\n9. **Convert frames back to RGB** and **save the magnified video**.\n\n---\n\n","metadata":{}},{"cell_type":"markdown","source":"## Key Equations\n\n### Eulerian Magnification (Conceptual Formula)\n\n$$\nI'(x, t) = I(x, t) + \\alpha \\cdot \\text{filtered}\\bigl(I(x, t)\\bigr)\n$$\n\n**Where:**\n- \\( I(x, t) \\): Original pixel intensity at spatial location \\(x\\) and time \\(t\\).  \n- \\( \\text{filtered}\\bigl(I(x, t)\\bigr) \\): The bandpass-filtered signal in the temporal domain.  \n- \\( \\alpha \\): The amplification factor that scales the filtered signal.\n\n---\n\n### Spatially Varying Amplification\n\n$$\n\\alpha_{\\text{level}} \n= \\frac{\\alpha}{2^{(\\text{pyramid\\_levels} - \\text{level} - 1)}}\n$$\n\n**Explanation:**\n- **\\(\\alpha_{\\text{level}}\\)**: The adjusted amplification factor at a given pyramid level.  \n- **\\(\\text{pyramid\\_levels}\\)**: Total number of levels in the Laplacian pyramid.  \n- **\\(\\text{level}\\)**: The current pyramid level index (starting from 0 at the bottom).\n\nThis scaling helps **reduce noise** at higher spatial frequencies by **decreasing the amplification** factor for finer details, while **lower pyramid levels** (coarser scales) receive **stronger amplification** to reveal larger-scale changes (like subtle head movements or breathing).\n\n","metadata":{}},{"cell_type":"code","source":"# ===============================================\n# STEP 8: MULTISCALE EULERIAN VIDEO MAGNIFICATION (With Resizing)\n# ===============================================\ndef eulerian_video_magnification(\n    input_path,\n    output_path,\n    freq_min=0.8,\n    freq_max=1.0,\n    amplification=50,\n    pyramid_levels=5\n):\n    # 1) Load video\n    video, fps = load_video(input_path)\n    nframes, h, w, c = video.shape\n    \n    # 2) Convert to YIQ for color fidelity\n    yiq_video = np.array([rgb_to_yiq(frame) for frame in video], dtype=np.float32)\n\n    # 3) Generate face mask from the first frame\n    original_face_mask = generate_face_mask(video[0])\n\n    # 4) Build Laplacian pyramid for each frame, store timeseries\n    pyramid_timeseries = [[] for _ in range(pyramid_levels + 1)]\n    for i in range(nframes):\n        pyr = build_laplacian_pyramid(yiq_video[i], levels=pyramid_levels)\n        for level_idx in range(pyramid_levels + 1):\n            # Resize face mask to match pyramid level dimensions\n            resized_mask = cv2.resize(original_face_mask, (pyr[level_idx].shape[1], pyr[level_idx].shape[0]))\n            resized_mask = resized_mask[..., np.newaxis]  # Ensure channel dimension\n            pyramid_timeseries[level_idx].append(pyr[level_idx] * resized_mask)\n\n    for level_idx in range(pyramid_levels + 1):\n        pyramid_timeseries[level_idx] = np.stack(pyramid_timeseries[level_idx], axis=0)\n\n    # 5) Apply temporal Butterworth bandpass filter + amplification\n    for level_idx in range(pyramid_levels + 1):\n        # Decrease amplification for higher spatial frequencies\n        alpha = amplification / (2 ** (pyramid_levels - level_idx - 1))\n        filtered = butter_bandpass_filter(pyramid_timeseries[level_idx], fps, freq_min, freq_max, order=3)\n        pyramid_timeseries[level_idx] += filtered * alpha\n\n    # 6) Reconstruct frames by collapsing the pyramid and converting back to RGB\n    out_frames = []\n    for i in range(nframes):\n        recon_levels = [pyramid_timeseries[level_idx][i] for level_idx in range(pyramid_levels + 1)]\n        recon_frame = collapse_laplacian_pyramid(recon_levels)\n        out_frames.append(np.clip(yiq_to_rgb(recon_frame), 0, 1))\n\n    out_frames = np.array(out_frames, dtype=np.float32)\n\n    # 7) Save the magnified video\n    save_video(out_frames, fps, output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.430615Z","iopub.execute_input":"2025-02-25T22:16:23.430816Z","iopub.status.idle":"2025-02-25T22:16:23.445307Z","shell.execute_reply.started":"2025-02-25T22:16:23.430798Z","shell.execute_reply":"2025-02-25T22:16:23.444665Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Usage Example\nAfter defining all the functions above, you can run the final pipeline by calling:","metadata":{}},{"cell_type":"code","source":"eulerian_video_magnification(\n    input_path=\"/kaggle/input/final-cv-dataset/face.mp4\",\n    output_path=\"magnified_face_final_optimized.avi\",\n    freq_min=0.8,\n    freq_max=1.0,\n    amplification=50,\n    pyramid_levels=5\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:16:23.446062Z","iopub.execute_input":"2025-02-25T22:16:23.446249Z","iopub.status.idle":"2025-02-25T22:17:10.947489Z","shell.execute_reply.started":"2025-02-25T22:16:23.446232Z","shell.execute_reply":"2025-02-25T22:17:10.946589Z"}},"outputs":[{"name":"stdout","text":"Saved magnified_face_final_optimized.avi\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink('magnified_face_final_optimized.avi')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T22:17:10.948295Z","iopub.execute_input":"2025-02-25T22:17:10.948609Z","iopub.status.idle":"2025-02-25T22:17:10.954005Z","shell.execute_reply.started":"2025-02-25T22:17:10.948580Z","shell.execute_reply":"2025-02-25T22:17:10.953309Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/magnified_face_final_optimized.avi","text/html":"<a href='magnified_face_final_optimized.avi' target='_blank'>magnified_face_final_optimized.avi</a><br>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Final Analysis of Eulerian Video Magnification Results\n\n## Overview\nThe Eulerian Video Magnification (EVM) pipeline was successfully applied to the provided facial video to enhance subtle temporal variations, primarily focusing on revealing physiological signals such as pulse. The comparison between the **original video** and the **magnified video** shows that the EVM implementation effectively amplified subtle color changes in the facial region, likely corresponding to blood flow patterns.\n\n---\n\n## Key Observations\n\n### 1. **Amplification of Subtle Changes (Color Pulsation)**\n- The magnified video frames exhibit **enhanced color pulsations** around the forehead and cheeks.\n- This effect indicates that the chosen **frequency range (0.8–1.0 Hz)** is appropriate for amplifying heartbeat-related signals.\n- The magnification appears natural without excessive flickering or distortion.\n\n### 2. **Black Borders in Magnified Frames**\n- The magnified video shows **black borders around the frame**, indicating a **mismatch in frame size** after reconstruction.\n- This issue likely arises from **improper resizing** during the Laplacian pyramid construction and collapse phases or from incorrect output dimensions during video saving.\n\n### 3. **Face Mask Accuracy and Region Amplification**\n- The **face mask effectively localized** amplification to the facial region, limiting changes in the background.\n- Minor leakage of amplification effects outside the facial region suggests that the mask could be **refined for tighter coverage**.\n\n### 4. **Temporal Stability**\n- The **temporal amplification is smooth and consistent**, indicating that the **Butterworth bandpass filter** was applied correctly.\n- There are **no signs of temporal jitter or sudden frame shifts**, reflecting a stable frequency filtering process.\n\n### 5. **Noise and Over-Amplification**\n- While the amplification is generally smooth, some **edges around the glasses and background areas appear slightly exaggerated**.\n- This may result from higher pyramid levels capturing unnecessary high-frequency details.\n\n---\n\n## Suggestions for Improvement\n\nAlthough time constraints prevent further modifications, the following adjustments could enhance the results:\n\n1. **Fix Black Borders:**\n   - Ensure that **frame dimensions remain consistent** during pyramid reconstruction by verifying the `cv2.pyrUp` resizing parameters.\n   - Adjust the **`cv2.VideoWriter`** settings to match the original video resolution.\n\n2. **Improve Face Mask Precision:**\n   - Apply **frame-by-frame face tracking** instead of using the mask from only the first frame to maintain mask accuracy in dynamic videos.\n   - Use **morphological operations** (e.g., dilation) to better define the mask region.\n\n3. **Control Over-Amplification:**\n   - **Reduce the amplification factor (`alpha`)** for higher pyramid levels to minimize noise in fine details.\n   - Consider **lowering `pyramid_levels`** if high-frequency artifacts persist.\n\n4. **Optimize Frequency Range:**\n   - If the target signal (e.g., breathing or pulse) varies, **adjust `freq_min` and `freq_max`** accordingly. For instance, breathing rates may require a lower frequency range (0.2–0.5 Hz).\n\n---\n\n## Conclusion\n\nThe implemented EVM pipeline demonstrates a **successful magnification of subtle physiological changes** within the facial region, with the chosen parameters yielding perceptible results. Despite minor issues such as **frame size mismatches** and **slight over-amplification**, the core objective of **revealing subtle temporal variations** was achieved. With the suggested refinements, the magnification results can be further improved to achieve clearer and more precise outputs.\n","metadata":{}}]}